{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import os \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlsPath = '/Users/sandeep/Downloads/Data5/boxlabel'\n",
    "imagesPath = '/Users/sandeep/Downloads/Data5/Image'\n",
    "images = os.listdir(imagesPath)\n",
    "xmls = os.listdir(xmlsPath)\n",
    "#filter imagess\n",
    "images = [i for i in images if f'{i[0:-4]}.xml' in xmls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbboxCoord(obj):\n",
    "    bbox = obj[1]\n",
    "    return [int(c.text) for c in bbox.getchildren()] \n",
    "def getCoord(result):\n",
    "    return [getbboxCoord(obj) for obj in result] \n",
    "def getlabel(image):\n",
    "    imageLabelPath = '/Users/sandeep/Downloads/Data5/boxlabel/' + image[0:-4]+'.xml'\n",
    "    imageXMLFile = etree.parse(imageLabelPath)\n",
    "    x,y = 0,0\n",
    "    #Get x\n",
    "    path = \"/annotation/object\"\n",
    "    result = imageXMLFile.xpath(path)\n",
    "    bboxs = getCoord(result)\n",
    "    return bboxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImagePath(image):\n",
    "    return '/Users/sandeep/Downloads/Data5/Image' + '/' + image\n",
    "\n",
    "def getLabel(image):\n",
    "    c = image[0:-4].split(',')\n",
    "    return [int(c[0]),int(c[1])]    \n",
    "def loadImages():\n",
    "    return [cv2.imread(getImagePath(i)) for i in images]\n",
    "def loadLabels():\n",
    "    return [getlabel(i) for i in images]\n",
    "\n",
    "#Loading Test Images and Lables\n",
    "imgs  = loadImages()\n",
    "labels = loadLabels()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeResults(fname,content):\n",
    "    basePath = '/Users/sandeep/Desktop/dataandmodles/data/playerTest/maskrcnn'\n",
    "    with open(f'{basePath}/{fname}.txt' , 'w') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_iou(boxA, boxB):\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = abs(max((xB - xA, 0)) * max((yB - yA), 0))\n",
    "    if interArea == 0:\n",
    "        return 0\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = abs((boxA[2] - boxA[0]) * (boxA[3] - boxA[1]))\n",
    "    boxBArea = abs((boxB[2] - boxB[0]) * (boxB[3] - boxB[1]))\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision_recall(image_results):\n",
    "    \"\"\"Calculates precision and recall from the set of images\n",
    "    Args:\n",
    "        img_results (dict): dictionary formatted like:\n",
    "            {\n",
    "                'img_id1': {'true_pos': int, 'false_pos': int, 'false_neg': int},\n",
    "                'img_id2': ...\n",
    "                ...\n",
    "            }\n",
    "    Returns:\n",
    "        tuple: of floats of (precision, recall)\n",
    "    \"\"\"\n",
    "    true_positive=0\n",
    "    false_positive=0\n",
    "    false_negative=0\n",
    "    for img_id, res in image_results.items():\n",
    "        true_positive +=res['true_positive']\n",
    "        false_positive += res['false_positive']\n",
    "        false_negative += res['false_negative']\n",
    "        try:\n",
    "            precision = true_positive/(true_positive+ false_positive)\n",
    "        except ZeroDivisionError:\n",
    "            precision=0.0\n",
    "        try:\n",
    "            recall = true_positive/(true_positive + false_negative)\n",
    "        except ZeroDivisionError:\n",
    "            recall=0.0\n",
    "    return (precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_image_results(gt_boxes, pred_boxes, iou_thr):\n",
    "    \"\"\"Calculates number of true_pos, false_pos, false_neg from single batch of boxes.\n",
    "    Args:\n",
    "        gt_boxes (list of list of floats): list of locations of ground truth\n",
    "            objects as [xmin, ymin, xmax, ymax]\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' (formatted like `gt_boxes`)\n",
    "            and 'scores'\n",
    "        iou_thr (float): value of IoU to consider as threshold for a\n",
    "            true prediction.\n",
    "    Returns:\n",
    "        dict: true positives (int), false positives (int), false negatives (int)\n",
    "    \"\"\"\n",
    "    all_pred_indices= range(len(pred_boxes))\n",
    "    all_gt_indices=range(len(gt_boxes))\n",
    "    if len(all_pred_indices)==0:\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    if len(all_gt_indices)==0:\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    \n",
    "    gt_idx_thr=[]\n",
    "    pred_idx_thr=[]\n",
    "    ious=[]\n",
    "    for ipb, pred_box in enumerate(pred_boxes):\n",
    "        for igb, gt_box in enumerate(gt_boxes):\n",
    "            iou= calc_iou(gt_box, pred_box)\n",
    "            \n",
    "            if iou >iou_thr:\n",
    "                gt_idx_thr.append(igb)\n",
    "                pred_idx_thr.append(ipb)\n",
    "                ious.append(iou)\n",
    "    iou_sort = np.argsort(ious)[::1]\n",
    "    if len(iou_sort)==0:\n",
    "        tp=0\n",
    "        fp=0\n",
    "        fn=0\n",
    "        return {'true_positive':tp, 'false_positive':fp, 'false_negative':fn}\n",
    "    else:\n",
    "        gt_match_idx=[]\n",
    "        pred_match_idx=[]\n",
    "        for idx in iou_sort:\n",
    "            gt_idx=gt_idx_thr[idx]\n",
    "            pr_idx= pred_idx_thr[idx]\n",
    "            # If the boxes are unmatched, add them to matches\n",
    "            if(gt_idx not in gt_match_idx) and (pr_idx not in pred_match_idx):\n",
    "                gt_match_idx.append(gt_idx)\n",
    "                pred_match_idx.append(pr_idx)\n",
    "        tp= len(gt_match_idx)\n",
    "        fp= len(pred_boxes) - len(pred_match_idx)\n",
    "        fn = len(gt_boxes) - len(gt_match_idx)\n",
    "    return {'true_positive': tp, 'false_positive': fp, 'false_negative': fn}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_scores(pred_boxes):\n",
    "    \"\"\"Creates a dictionary of from model_scores to image ids.\n",
    "    Args:\n",
    "        pred_boxes (dict): dict of dicts of 'boxes' and 'scores'\n",
    "    Returns:\n",
    "        dict: keys are model_scores and values are image ids (usually filenames)\n",
    "    \"\"\"\n",
    "    model_score={}\n",
    "    for img_id, val in pred_boxes.items():\n",
    "        for score in val['scores']:\n",
    "            if score not in model_score.keys():\n",
    "                model_score[score]=[img_id]\n",
    "            else:\n",
    "                model_score[score].append(img_id)\n",
    "    return model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_precision_at_iou(gt_boxes, pred_bb, iou_thr=0.5):\n",
    "\n",
    "    model_scores = get_model_scores(pred_bb)\n",
    "    sorted_model_scores= sorted(model_scores.keys())\n",
    "# Sort the predicted boxes in descending order (lowest scoring boxes first):\n",
    "    for img_id in pred_bb.keys():\n",
    "        arg_sort = np.argsort(pred_bb[img_id]['scores'])\n",
    "        pred_bb[img_id]['scores'] = np.array(pred_bb[img_id]['scores'])[arg_sort].tolist()\n",
    "        pred_bb[img_id]['boxes'] = np.array(pred_bb[img_id]['boxes'])[arg_sort].tolist()\n",
    "    pred_boxes_pruned = deepcopy(pred_bb)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    model_thrs = []\n",
    "    img_results = {}\n",
    "    \n",
    "    img_ids = gt_boxes.keys()\n",
    "    for img_id in img_ids:   \n",
    "        # Recalculate image results for this image\n",
    "        img_results[img_id] = get_single_image_results(gt_boxes[img_id], pred_boxes_pruned[img_id]['boxes'], iou_thr=iou_thr)\n",
    "# calculate precision and recall\n",
    "    #print(img_results)\n",
    "    prec, rec = calc_precision_recall(img_results)\n",
    "    precisions.append(prec)\n",
    "    recalls.append(rec)\n",
    "  \n",
    "    precisions = np.array(precisions)\n",
    "    recalls = np.array(recalls)\n",
    "    prec_at_rec = []\n",
    "    for recall_level in np.linspace(0.0, 1.0, 11):\n",
    "        try:\n",
    "            args= np.argwhere(recalls>recall_level).flatten()\n",
    "            prec= max(precisions[args])\n",
    "            print(recalls,\"Recall\")\n",
    "            print(recall_level,\"Recall Level\")\n",
    "            print(args, \"Args\")\n",
    "            print(prec, \"precision\")\n",
    "        except ValueError:\n",
    "            prec=0.0\n",
    "        prec_at_rec.append(prec)\n",
    "    avg_prec = np.mean(prec_at_rec) \n",
    "    return {\n",
    "        'avg_prec': avg_prec,\n",
    "        'precisions': precisions,\n",
    "        'recalls': recalls,\n",
    "        'model_thrs': model_thrs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_boxes= {\"img_00285.png\": [[480, 457, 515, 529], [637, 435, 676, 536]]}\n",
    "# #Pred Boxes\n",
    "# pred_boxs={\"img_00285.png\": {\"boxes\": [[330, 463, 387, 505], [356, 456, 391, 521], [420, 433, 451, 498], [328, 465, 403, 540], [480, 477, 508, 522], [357, 460, 417, 537], [344, 459, 389, 493], [485, 459, 503, 511], [336, 463, 362, 496], [468, 435, 520, 521], [357, 458, 382, 485], [649, 479, 670, 531], [484, 455, 514, 519], [641, 439, 670, 532]], \"scores\": [0.0739, 0.0843, 0.091, 0.1008, 0.1012, 0.1058, 0.1243, 0.1266, 0.1342, 0.1618, 0.2452, 0.8505, 0.9113, 0.972]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.] Recall\n",
      "0.0 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n",
      "[1.] Recall\n",
      "0.1 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n",
      "[1.] Recall\n",
      "0.2 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n",
      "[1.] Recall\n",
      "0.30000000000000004 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n",
      "[1.] Recall\n",
      "0.4 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n",
      "[1.] Recall\n",
      "0.5 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n",
      "[1.] Recall\n",
      "0.6000000000000001 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n",
      "[1.] Recall\n",
      "0.7000000000000001 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n",
      "[1.] Recall\n",
      "0.8 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n",
      "[1.] Recall\n",
      "0.9 Recall Level\n",
      "[0] Args\n",
      "0.14285714285714285 precision\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'avg_prec': 0.12987012987012986,\n",
       " 'precisions': array([0.14285714]),\n",
       " 'recalls': array([1.]),\n",
       " 'model_thrs': []}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_avg_precision_at_iou(gt_boxes,pred_boxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
