Proejct Goals:
    - Ball detection (DONE)
        - Find out why the predictions so shit?
        - Does it differ If I create my own dataset?
    - Action Classification (DONE)
        - Find out why the prediction are so shit?
        -Does it differ If I create my own dataset?
    - Player Detection 
    - Start on team seperation 
    - Court and board Detection
===================================================================================================================================================
===================================================================================================================================================
===================================================================================================================================================
May Experiment with these in future:
- https://towardsdatascience.com/detecting-soccer-palyers-and-ball-retinantet-2ab5f997ab2


Project 
    - What other methods are avialable to detect ball??
        - Resenet50 CNN regression?
            - What does the dataset format look like? (Image and points)
                - A points file txt file
                    - With same filename
                    
            - What is a datapoint going to be?
            - Are there any such dataset ot there?
                -http://ipl.ce.sharif.edu/ball_datasets.html
                - http://basketballattention.appspot.com/
            - How is the process of collecting the dataset?

Download videos
    - Take screen shots 
    - Label them


- I am trying to test CAT regression on a single image 
    -  export the export.pkl file 
    -  It contains the following
        - (the model, the weights but also some metadata like the classes or the transforms/normalization used).
    - Yes, Restnet is able to predict where ears and nose even in occlusion.

STEPS:
        -  Create the learner
        -  Load the weights
        - Export the learner


Creating Basketball DataSet
    - Get frames at given timstamps
        - x1000 to get timestamp in mircoseconds
        - Convert it to seconds
        - https://www.youtube.com/watch?time_continue=SECONDS!!!!!&v=VIDEOIDDDD
            -example: https://www.youtube.com/watch?time_continue=2805&v=AALM9_RgV6c
        - Is there a way I can extract out the frame form youtube videos?
    - Given an Screen shot I know where the ball is
    - Get screen shots
        - Get a screen shot using code (DONE)
            - Login to youtube 
            - Use the page to search for a vidoeo's screen shot 
        -Get screen shots of the sent urls 
            - resize the image in JS
            - Redirect the chrome downloaded item to a dataset folder
            - Build a function that send a JSON STRING
                - REQUEST STRING:
                        - '{links:{0:'link1',1:link2.....}}
                        - '{links:'link1,line2.....'}' (USING THIS)
                    - What is the best way to send this data
                        - We need to use a middleeware to be able recieve data outside params 
                            - var bodyParser = require('body-parser');
                            - app.use(bodyParser.urlencoded({ extended: false }))
                            - data will be available on req.body.links
                            - app.post() will take aynsc function
                - JSONIFY THEM 
                - PROCCESS EACH LINK 
                - SEND THE IMAGE
                - REAPEAT UNTIL ALL THE LINKS HAVE BEEN PPROCCESSD!


Training Images I am not sure about
    - 164,31.png
        - I can get the path of an Image
            - data.train_ds.x.items[idx]
        - How to gwt the idx?
        - Lets try to learn anyyway, 
            - Itf the result is really bad, we will go and fix the lable using opencv 

Task 
 - Train the model in larger images size(Done)
    - Acutal data size: 490x360(Done)
    - Can I get any sort of x and y coords form the fast ai
        - Maybe look up how image.show(y=ImagePoints), takes imagespoints and finds the x y coord to find the dor
            - Imagepoints: Potential target to be superposed on the same graph (mask, bounding box, points) 
                - So its like a sketch paper on top of an image, its is the same size as the image
                    - So may be find what coord holds the red dot in the image points?
                        - Inside image points, Coords are scaled to range (-1,1), iff SCALE== true
                            - Find out how it scales the coords and reserve it by tracking what happens SCALE==TRUE?
                                - scale_flow() function gets ran
                                    -https://github.com/fastai/fastai/blob/master/fastai/vision/image.py#L251 LINE:440 (NEXT)
                                    - to_unit = True when scale_flow is true
                                        - flow.flow = flow.flow/s-1
                                            - I guess flow.flow are the coordinates
                                             - s = flow.size[0]/2 , flow.size[1]/2
                                             - What does the flow size look like relative to the image? Is it shape of the image?
                                                - imagePoints.size return flows size, which are (360,490)
                                                - s seems to be the center coord of the image

- Run fastai in my pc (DONE)
    -https://medium.com/@plape/how-to-install-fastai-on-mac-a05496670926


- Mapping the coords given by the model for image size (490,360) to orginial image coord (DONE)
    - https://stackoverflow.com/questions/25684327/how-to-find-an-equivalent-point-in-a-scaled-down-image
    - Draw a cricle around the original image
        - coordGiven: y,x
        - originalImage: x',y'
            - Find scaleFactor: sizex'/sizex..
                - NOTE: opencv: (width, height)
        - NewCood: x *  scaleFactor
    -NEXT: COMPLETE THIS!



The cricle is not drawn in the right part oof the image
    - Even when I cchange the center, the cricle is not chaingein it place
    - The originalCoord is wrong
    - Check if the given unscaledCoord is correct
        - Try drawing a circle in a random image
        - Its wrong: Becuase I forgot to divide s coord by /2


I getting Confused with W and H 
    - image.size = (360,490) = (H,W)
    - image.data = [-0.46 , -0.25] = (H,W)
    OPENCV
    - img.shape = [360,490]  = (H,W)
    - cv2.resize(img , (490,360)) = (W,H)
    - cv2.cricle(img, (490,360)) = (W,H)]
    - img[row ,col] : [H,W]
I have an array of image -> FastAi Image Object (NEXT)

Detection on a video
    - Pause 

Progees: 
    - I can take. picture and do regression on the image to the location of the ball
NEXT: 
    - However, the predictions of the ball is not quite accurate
    - I will try fixiing the labels 
    - Train it with more images?

Ball detection
--------
Action Classification
- Classification model
    - I will trey this 1st
- Posenet
    - The idea is to get latent features from pose net and do action detection 

Classification model
    - I need to make a dataset
        - I need labels, look at the csv file
            - The second last column has the classes
                - 3-pointer success(DONE) (CHECKED)(CHECKED)
                    - train:348(need more)(880)
                    - val: (DONE)
                -3-pointer failure, (DONE)(CHECKED)(CHECKED)
                    - train:701
                    - val: (DONE)





                -free-throw success,(DONE)(CHECKED)(CHECKED)
                    - train:221 (NEED MORE)(542)
                    - val: (DONE)
                - free-throw failure, (DONE)(CHECKED)(CHECKED)
                    - train:135 (NEED MORE)(339 total)
                    - val: (DONE)


                - layup success, (DONE) (CHECKED) (CHECKED)
                    - train: 408 (NEED MORE)(1189 total)
                    - val:(DONE)
                - layup failure,(DONE)(CHECKED)(CHECKEd)
                    - train: 517
                    - val:(DONE)
                - other 2-pointer success, (DONE)(CHECKED)(CHECKEd)
                    - train:409 (NEED MORE)(1017 total)
                    - val: (DONE)
                - other 2-pointer failure, (DONE)(CHECKED) (CHECKED)
                    - train:726 (DONE)
                    - val: (DONE)
                - slam dunk success: (DONE) (CHECKED)(CHECKED)(might need a bit more valid more on validation set)
                    - train:108 (NEED MORE) (279)total
                    - val: (DONE)
                - slam dunk failure: (DONE)
                    -train: 18 (NEED MORE)(46 total)

                - steal success   
    - The video links are going to be same but the label links has to change
        - Check how I get ball possition 
        - I can just access it using row['actionSuccess']
         - if I just name them with the classname, the image file is going to replace each other
            - need some kind of unique
                - index_actionClass.png: 1_shoot.png
                     - The index is relative to the main big table not the cut out training and val table
                        - Downloading training images into: /Volumes/My Passport/FinalYearProjectData/ActionClassification/train
                        - Got an error in server when downloading for this link https://www.youtube.com/watch?v=cr3caxj-4E4&t=46m17s
They way I downloaded the dataset is not helpful
    - I need to be able to see how many image each classes have 
    - I need to download 100 more image extra just incase I remove due to falulty images

Reorganise the data in a good format
    - Take a look I orginsed the data for other classification 
        - Inside the data folder
            - I have classes folder (DONE)

TODO:
    - I will concatonate slam dunk success and failure class
    - There are alot of flase labels
    - There are 2 options
        - Download the data set for each class 1 by 1 agian 
        - Download only from the point of last image 
            - Make a new dataframe from oldlocation to new loaction 
                - Download thoes images
                -example: train_links_df[train_links_df['actionSuccess'] == 'slam dunk success'].loc[8852:]
        - can I fix timing issue?
            - what is causing the timing error?
                - I am lookin at 5 dummy links for time lag
                    - first image, timing is correct, its taken at the end of the second 
                    - second image, timing is wrong, given:40 and but taken at 42 (2 second later)
                    - third image, timingi is wrong, given:6 but taken at early 8 (2 second later)
                    - fourth image, timing is wrong, given:7 ubt takeb at early 8 (1 seond later )
                    - fifth imag, timing is wrong, given 46 take at 48 (1 second later)
                - Its seems that for a most links it is taken after 2 seconds
                - Is there a to get more specific timings?
                    - Try: pausing the video?
                        - pausing the video gets better results, 
                        - next: I might have to wait a bit longer so that the pause mark goes away
                    -Try: I am trying to if in get_time if I choose between option of round down or up
                        -currently i have -1, which tell me I it is giveing me 1 second back
                            - What happenns if I remove this 1 second, and give better rounded sec
                                - just round: lead to bad reuslt
                                - round - 1 : bad results
                                - round - 2 : better results
                                    - When was the actually take comppared to the orginiaail 
                                        - Because we are using round, it is much closer to original time
                                            - 54:53
                                            - 42:41
                                            - 8:7( 1 second delay is good because @8 player has already ladnded)

                                - round - 3 : video has go to much backward in time 
                        - Hence, I will just stick to round()-2

                    - Each data is going propely labled with the following info
                        - ballPos
                        -idnex
                        -name
    - Cleaning up data from each class (DONE)
        - Remove image that clearly should not be there
    - Training the data 

    - Some classes look very similar, what soloution is there?

------------
MERGing Classification model and regression model together for demo
    - Dont forget to convert shit to rgb 
        -test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)
    Regression Model Class:
        - getPrediction()
        - drawPrediction()
    Action classification
        - getPrediction()
    - Need to load the 2 notebooks
    -Side note: Classes are in this order
       - ['3-pointer failure',
          '3-pointer success',
          'free-throw failure',
          'free-throw success',
          'layup failure',
          'layup success',
          'other 2-pointer failure',
          'other 2-pointer success',
          'slam dunk']
-------------


Detecting Teams
    - What does mask of specific colours mean?
        - Masking means: 
            - Cutting  out only the portion of image that you need and placing it an empty black image template
            - Hence, the resulting image will be same as the image, in a black background
            - Just like Masking in Final Cut Pro
    - Does detecting teams depend on detecting player first?
        -  we extracted bounding box from tensorflow object detection 
        -  count the percent of pixels in that bounding box that are non black to decide the team for that player.
        - 


---------
 Player Detection
    - What is the best model to do player detection?
        - Measurement they use is mAP
            - Naming Convention
                - Negatives
                    -  Wrong Y 
                - Positives
                    - Acutal Y
                - True
                    - Correctly Classifying
                    - Arrow is going the correct direction
                - Flase
                    - Incorrectly Classfying
                    - Arrow is going the wrong direction
            - precision 
                - if it does predict y as Y: how likely is it that its the top arrow (accurary of top arroow)
            - recall
                - probabilty that it find the (y=y) how good you find all the positives.
            - IOU
-----------------------------
REDO 

Ball Regression and Action classification
    - Create 1 dataset that serves both 
        - Collect yourself 
            - Downlaod images 
                - 2 Points
                - Layup 
                - Dunk
        - Search  
            - Google?
            - Github?

Questions:
    - What did the Yolo-Basketball detection dataset look like?
-------
Presentation:
    - 10 minutes
    - Title slide
        - Project title, name, course, data 
        - Outline
    - Problem description 
    - Motivation
    - Approach
    - What you have commplished so far(Not mars givens)
    - Analyisis (how will you judge the outcome of your work)
    -A conlusion slide (what did or will you accomplish)
    - What still needs to be done?
    -Clarity of explanation of the deliverables 
    - How will the project be evaluated?


Possibile themes 
    - https://slidesgo.com/theme/data-waves
    - https://slidesgo.com/theme/ai-tech-agency
    -https://slidemodel.com/templates/basketball-court-game-plan-powerpoint-shapes/


QUESTIONS?
    - WHAT IS HOI?
        -The Human-object interaction (HOI) detection is the task of localizing and inferring relationships between a human and an object.
        -  For example, detecting the HOI “human-row-boat” refers to localizing a “human,” a “boat,” and predicting the interaction “row” for this human-object pair. 
    - Why I choose HOI on basketball
        Algorithmic advantages of doing HOI in basketball
        - The number of objects in the object space is limited
        - The action is some what related 

SCRPIT
    laaning an Preperation 
    - Preminalry preperation: instead of for fun.
    - Over the christmas break in my second year, I did my very first  deeping learning project every. More like a helloWorld project
        - It was about cats vs dogs recognition    
        - Here is me, telling every one in the world how I did the projct and just telling people the how happy on making a computer recognise cats and dogs
            - I was expecting people to be like: oh it took you a week to do this piece of cake, it takes me 30 mins.
            - But nope: responses that I received was very encouraging
    - So with newly. gained confidence, I moved on and  did the same thing over different dataset over and over aigan with minor varitations,
        - Some of the cool dataset that I got my hands dirty with is the minist and cats facial feature 
        - Minist was cool becauuse It was a multi-classification problem meaning there were more than 2 classes to predict from.
        - Cats facial feature prediction is fav ...
            - It is a regression problem, it where you take an image and have the superpower to predict a continous value 
            - The model was doing great.
            - But I was like what happens, if I block of some of the features,forexample, if I remove the cats eye  will it still be able to predict where the eye is??
                - So here are some results. When I saw this, I was like what NO WAY. It's so dope, its like if the the model is reallly good at throwing a dart with half blind. 
    - So Why am I telling you all these? Its because, these are the preliminary project and experiements for my 3rd year project. And form the back of my main 3rd project. 

    Motivation
        - At this points, I am getting bored. Cause, all I am constally doing interperting an image via object bounding boxes and Nothing else.
        - Wouldn't it sick though, if your model could understand what's going on in a image. Perhasp, extract semantic information out of the image.   
        - You know after a basketball game, players have to sit hours of flim session to review the game with a tv remote controller. I mean remote controller. At this day an age you are tidously using a remote controller . Hold up I got something better for what if I give you a system where you type in layup and it retrieves you all the clips of you laying up. 
        - Focusing creative engery 
        - There are other advantages  camera planning, image captioning. 
        - If solution is to exist, it can def improve enjoymnet of the sport. 


    Problem Description
    - So the umbrella term for my project is called Human-Object interaction recognition detection via Machine learning 
        - I know it a mouthful but in laymans terms: All it means is just find where the human is, find where the object of interest is. And assing them a verb. Therefore, the goal is to represent it  human object interaction as a triplet (SLides eefect draw boiund boxes)
        - This problem is hard because, you will have an exponentail expolsion here.
        - So for this project, I am trying to slove HOI in the basketball domain. The size of the object and human space is relativley relaxed.
        - The object space only contains 1 object and that is the ball
        - Well for the human space, we dont have consider the spectators, cameramans , ref so The human space is shirnked down into only the players
        - So that a thumbs up, as far as my time and  computational resources required
            - Meaning less dataset required
            - Lower model training time
            - Which means , I have more time to improve and compare model with others and do alot of other experiments
    - However, HOI with basketball has it own unqie set of challanges 
        - I extermely hard to localise the basketball. 
            - First, believe me when I say the ball appears really small and looks like an organe dot and just uses too few pixles. 
            - Plus, when it moving, I not even a dot anymore,  so it appears blurred out. And if the player is moving with it too fast, it dissapears. 
            - Sometime, it dissapears because it is occluded by players
        - Player interact with object in various ways that look very similar. 

    Apporach
        - So How do we tackel this problem?
        - So lets bring back the computer scientist mentality here: Lets break the problem down to smaller subproblem. And lets slove the subproblem. Which will inturn slove the main problem
        - The sub problems are 
            - Localizing  the player
            - Localizing the ball
            - Classifiing the verb
        - Well
            - Localizing  the player
                - Its th reasy part. Use pre-trained  model like YOLO to detect humans and filter the detecting them using  computer vision teq like thresholding
            - Localizing the ball
                -  Lets use a regression model, to interpolate the spatail coords just like we did with cat facail featuers 
            - Classifying the verb
                - Well we can use multi-classification model just like we did with minsit. Where 1 action is a 1 class  
    Deiliverabls
        - So that when I have done all the stuff, I want to be able to take a video and analyze frames and report on what the player is doing with the ball
        - And the I want to go the extra mile, detecting teams to produce action stats sheet for each team. So say like, Lakers only have taken  3 pointer, the def need to .
    Evaluation
        - Quantative Evaluation will include, evaluating confusion matrix 
            - This is  the confusion matrix, of my model that classify verb. As you can see there 
            - At this point there are alots of false postives and flase negatives
                - between classes that look similar such as training  
        - Then I will be evaluating its Percsion and Recall Scores
            - to find out how accurate the prediction are and how often model detects a certain action
        - Becuase percsion and recall are both inportant, I will be evaluting the models  F1 score as well.  

            
---------
New ideas:
    - Time Compression
        - Assumes that the camera is stationary?
        -  That would certainly create multiple, and cumulative instances of the same subject on the same frame. 
            -Since I cannot turn on and off the sun, I am going to use a light source that can be controlled in that fashion: Canon Speedlites will offer that capability very nicely.
            - Exposure data is 0.5 sec at f-8. During that half-second exposure, the flash that was mounted on the hot shoe of the camera, a Canon G9, fired multiple times to register the number of images you see here.
    - What happens if I add mutliples frames in to 1
        - Experiment:
            - Combining All the frames in the clip
                - mergedImage = w * oldImage  + w2 * newImage
                    - Increasing w2 brigthens up 
How to draw key points?(Complete)
    - https://github.com/luxifeo/posenet-python/blob/master/posenet/utils.py
    - uses 
        -out_img = cv2.drawKeypoints(img, cv_keypoints, outImage=np.array([]))
        - what is cv_key_points? (DONE)
            - Its is list of something
                - something: cv2.keyPoints object
                    -cv2.KeyPoint(kc[1], kc[0], 10. * ks)
                -What is kc[1] and kc[0]?
                    -kc = key_coords.
                    -x= kc[0] and y=kc[1] or the reverse
                - Note: Every point comes with
                    - (score, part, position)
                - what is ii?
                    - index of score
                - what is ks?
            - This function seems to give out empty image, Who calls this function? Do they fill it up latee
How do I draw skeleton? (DONE)
    - What does, get_adjacent_keypoints(keypoint_scores[ii, :], keypoint_coords[ii, :, :], min_part_confidence) do?
        - What is posenet.CONNECTED_PART_INDICES?
            - imported from posnet.constants
            - What is PART_IDS?
                - {pn: pid for pid, pn in enumerate(PART_NAMES)}

        - What is results?
            - Its an array of array. [ [leftPart , rightPart],....otherpairs]
            - pair = np.array([keypoint_coords[left][::-1], keypoint_coords[right][::-1]]).astype(np.int32)
            - What does [::-1] do? Its reverses lefpart: (x,y ) -> (y,x)
            - What is left and right? 
                - Its an index of where of where left and right respective keypoint is  found
        - For 1 pose/keypoint/{part:leftLeg} . it find the counter part rightLeg coord and return a [{x:,y:} , {x:,y}]
        - What does  keypoint_coords[ii, :, :] do? 

How do draw keypoints and skeleton? (Complete)

How to convert array of images to base64? (Complete)
    - https://stackoverflow.com/questions/16065694/is-it-possible-to-create-encoded-base64-url-from-image-object
    - https://stackoverflow.com/questions/40928205/python-opencv-image-to-byte-string-for-json-transfer (WORKED)
Learn about heatmaps? (Complete)
    - Read abit 
Read about the mouse project?(Complete)
    - Encode mouse movements, directions, speed and acceleration of mouse in a consistent way
        - Allowed to encode trajactory lines
- Produce an image containing all the poses off all the frames in an image (COMPLETE)
    - On a black image (DONE)
    - On a non black image(DONE) 
        - 1 second --> 60 frame

- Produce an image containg all the poses of all the frames in an image and color code it (Complete)
    - How to map numberOfFrame with colour
        - Learn the different colour spaces avialable?
            - What is a colour space?
                - three-dimensional object which contains all realizable color combinations.
            - What are the different colour spaces avialable?
                - HSV
                    - (hue, saturation, brightness)
                        - hue: which colour(expressed as a number from 0 to 360)
                        - saturation: pruity of the choosen choosen colour (toward zero introduces more gray and produces a faded effect , less pure more gray)
                        - value:  brightness or intensity
    - Example of colou mapping
        - learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/
            - What they are doing is mapping one colour to another colour
                - turns it into an grayImage 
                - uses applyColorMap() to map the image to anthoer color 
            - NEXT:::What I want is map a frame count to another colour
                - Then use this colour to draw the poses
                - The problem: Mapping interger into RGB/HSV space?
                    -  Does cv2.polylines accept 

How to map an integer in range 0-60 to a color spectrum?  (Complete)
    - Steps
        - Get the perctange value (Normalize)
         - Then the number is mapped to a color using a subclass of colormap
            - LinearSCM
            - ListedColorMap
    - Tutorial:
        - https://matplotlib.org/tutorials/colors/colormap-manipulation.html
    - Returns RBGA 
        - what does cv2.polylines and drawKeyPoints accept?
            - RGB or BRG
            - Do they accept fractions?
                - BRG
                - Does not accept fractions has to be in the range 0-255
Debug
    - It is only using yellow color
        - (90, 7, 7) then (37, 231, 231) and the last color is just repeated
    - mapINTocoolours is geting negative numbers (totalColors)
        - totalFrames is beign pass as totalColours
            -Why is totalNumberFrames so big?
                - -230353946974395
                - videoPath: ._Free-throw39.mov

Try to play around with only darwing fewr frame and see if that make a differet (COMPLETE)
    - There are 58 frames => 2 seconds 
    - 29 frames => 1 second
    - Tried:
        - Every 10 frames (Time need)
        - Every 5 frames (Time)
        - Every 2 frames (Time) 
 - Collect dataset (COMPLETE)
    50%-30% split
    50 Train 
    15 Valid
    - Test classes:
        - 3-Pointer (DONE)
        - Free-throw 
        - Layup
    -Download the videos 
        - There are 2 way
            - I can 1 long video with many 2 seconds 3-pointers video
                - May have problems
            - I can make 3-pointer video clips direclty
                - Try this
            - Linked Used
                -Free-throw
                    - https://www.youtube.com/watch?v=7tW3oYk43yg
                    - https://www.youtube.com/watch?v=qawlTEmBEns
                        -NBA Airball Free Throws Compilation

- Next:Train the on new images 
- Can I label the hand differnt color?
- I notice there are some consistency in the basketball court 
    - The drawing on the ground is always the same
    - The inner box has one dominiant color
    - There is always the stateframe writing in the rim
- May be I can uses statiatical patterns of feature poins to recognise categgories of objects?
    - May be I can do feature points macting 
        - Do SIFT Feature Matching
            - Given a feature in l1 (STATE FRAME),how to find the best match in l2?
                - I am gonna go make some notes on SIFT Feature Matching now

What does surf dectection.py contain?
    - line 234 has drawKeyPoints
        - At this stage we must have done the processing already
        - feature_object.detectAndCompute()
            - What is feature_object?(DONE)
                - it is created by
                    -  cv2.xfeatures2d.SIFT_create(400) or by 
                        - 400 is the hessiabThreshold
                        - Wtf is hessian threshold?
                    - cv2.ORB_create(800)
                - It is an Feature2D object class, that extracts features
                - What does .detectAndCompute()
                    - takes image/crop and mask
                        - What is crop?
                            - basically the region with feature that you want to detect
                    - returns keypoints_cropped_region and descriptors_cropped_region
                        - I understand what keypoints are
                            - It tell you  where the feature is located at
                        - What is a decriptors_cropped_region?
                            - decriptors are the actually feature values
    - Thee feature mapping is happening in Feature Matches window
        - Maybe I should look there?
        - it is using cv2.drawMatches(crop,keypoints_cropped_region,frame,keypoints,good_matches,None,**draw_params)
            - crop(objectImage) , keypoints_cropped_region(featurePoints in the main object image)
            - frame(current Frame), keyspoints (I think it find feature points just like crop and set this var)
            - good_match(matches1to2)
                -DMatch object: Object for matching keypoints
        - Is keypoints gernerated how I think it is? 
            - Yes: a gray_frame is passed
        - How is good_matches created? (DONE)
            - it is a list of tuples (m(features/descriptors in objectImage) , n (features/descriptors in training image)):
                - we are filtering tuples(m,n) in matches list 
                    - based on: m.distance < 0.7*n.distance
                - How is matches list created?
                    - matcher.knnMatch(descriptors_cropped_region, trainDescriptors = descriptors, k = 2)
                        - Returns you (m,n) which are matching features (matchobject)
                    - What is a matcher?(DONE)
                        - is returned from cv2.FlannBasedMatcher(index_params,search_params) or cv2.BFMatcher()(Brute Force Matcher) depending on cv2 version
                            - The returned object is a DescriptorMatcher class
                            - (Class for matching keypoints descriptors)

                        -  The frist one only works for OpenCv > 3.1
                            - We have 3.4.5! We can good to go!
                        - WTF is index_params and search_params?
                            -  index_params: 
                                - KDTREEPARAM in the form of dictionary: 
                                    -dict(algorithm = FLANN_INDEX_KDTREE, trees = 1)
                            - search_param
                                - Number of searches maybe?
                                    - another dict:  dict(checks=50)
                    - Finally matches between the 2 features Steps
                        - matcher.knnMatch(descriptors_cropped_region, trainDescriptors = descriptors, k = 2) 
                            - trainDescriptors = descriptors: is the descriptors/features from the actually image
        - Wtf is findHomography? (DONE)
            - homography: matrix transform
                - Find matrix transfromation from 1 set to another using RANSAC
                    - HUH? WHAT DOES RANSAC DO?
                    - If you want to map the objectImage to the frame you got 
            - cv2.findHomography(source_pts, destination_pts, cv2.RANSAC, 5.0)
                - What are source_pts?
                    -array: np.float32([ keypoints_cropped_region[m.queryIdx].pt for m in good_matches ]).reshape(-1,1,2)
                    - wtf is queryIdx?
                        - it bascially store the index of feature in the orignial list 
                    - keypoint.pt gives a point object (coords of the key points)
                        - I can access x and y by pt.x , pt.y
                    - what does .reshape(-1,1,2) do?    
                        - Fit the data in (1,2) and you can have as many rows as you want
        - How does he get the statistical fit?
            - Get the points objects in the frame
                - np.float32([ keypoints[m.trainIdx].pt for m in good_matches ]).reshape(-1,1,2)
            - what does cv2.fitEllipse() do?
                - find the best Ellipse for the object
                - Uses this to draw the ellipse on the frame
                    - cv2.ellipse(frame, ellipseFit, (0, 0, 255), 2, 8)
    - How does he draw matches?
        - what does cv2.drawMatches()? 
            - returns an image


STEPS:
    - Draw surf/sift keypoint on an image (
        - we need xfeature2d
         - Could not install it 
        - using external github project
            - Takes to much time 
                - img (30sec)
                    - 195*164
                - img2 (5 mins)
                    -1000x700 (aboutx)

    - Draw orb keypoints on an image (DONE)
    - Do feature matching in 2 image (DONE)
        - Not detecting feature properly

GOAL: DETECTING COURT LINES AND BOARD:
     Do canny edge and then feature matching?

    Try good featuers to tack?
    Do canny on a single image(Done)
    Filter edges in canny
        - Did really bad
    Look into Hough Lines Detection
        - The following paper detects the main court lines 
            -https://web.stanford.edu/class/ee368/Project_Spring_1415/Reports/Cheshire_Halasz_Perin.pdf 
            - Change colour space (Done)
            - Convert to H-Space (Done)
            - Create binary model
                - Alot of pixel values are being set to 0 
                - They dont pass the threshold value
                    - setting threshVal = 40 works
            - Perform erosion and dialiaton(DONE)
            - Perform Canny edge (DONE)
            - Hough transform in order to detect the line
        - THe following paper is doing what I want
            - https://www.cs.ubc.ca/~murphyk/Papers/weilwun-pami12.pdf
                - They match the court model with the edge of the video frames to establish correpondence
                    - What is a court model?
                        - - What does it mean to match edge of the vide with court model?
                            - The edge of the video frames are computed using Canny detector
                                - You have to specify firt frame homography? How do I get this?
                    - What does establishing correspondence mean?
        - The following paper is doing court detection has code for it 
            -https://github.com/stephanj/basketballVideoAnalysis/blob/master/court-detection
            - houghlines methods just does not work too well
            - Trying the autoencoder way
                - What do I have to build and perpare for this to work?
                    - WTF is a regularised autoencoder?
                        - Main idea: Regularize the rescontruction of the autoencoder by introducing more stuff in its loss fuction 
                            - Basically, controll what it reconstructs
                                - Example, denosing stuff. We are controlling what it reconstructs. 
                                - We dont want it to just learn to replicate



    - Detection by dominant colour
        - Court colour is the most domninant color (DONE)
        - The change in the color 


Understand basic of features 
    - Feature unqinuess
        - High change in in all direction
    - Types
        - Harris feature points 
            - Does really well with Rotation and translation
            - Bad with scale
        - SIFT 
            - Good with scale
            - Hesassian matrix is talke about when Feature Point Filtering is mention
                - H: Something to do with eigenValues and eigenvectors
        - SIFT feature matching
            - We need to compare features between 2 images 
                - Uses k-D trees 

The court Detection and the board detection can be solved using Imagesegmentation model
    - Why would this solve the problem?
        - The segment can tell us where the board is 
        - Each section of the court etc 
    - What kind of dataset do I need to build?
         - x: image
         - y: imagemask
            - Where pixles turn to unqiue integers for class.
            - Dont conufuse your self. fastai open mask will color code it for you 
        - codes.txt
            - where index of the item refer to the specific item 
            - exmaple. [food, water , pc , ...]
                - in your mask your should have pc pixels labeled as 3

    - What tool do I need to make the data set?
        - How to install the tool?
            - Try pixelAnnotationTool : https://github.com/abreheret/PixelAnnotationTool/releases
                - Cant download 
            - Try: https://image.predicted.ai/app
                - Can not download the images/ Works in icognito
                - Files to segment
                    - Train
                        - 2-Pointer 
                            - #13e9d5
                        - 3-Pointer
                            - #f40930
                            - TrainFolder:(DONE)
                            - ValFolder: Dpme
                        - Dunk
                            - Train:(DONE)
                            - Valid: (DONE)
                        - Layup
                            - #1a07e7
                            - (DONE)
                        -Other
                            - #c404e5
                        - Board 
                            - #1fe40c
                        - Freethrow
                            - #d3da00
                    - Valid 
                        - 3 Pointer
                        - Dunk
                - All the lables file go to Labels folder
        - What folder setting do I need for the data?
            - This the set for camvid from youtube video
                - https://www.youtube.com/watch?v=MpZxV6DVsmM&t=193s: @1:03:41
                - data/camvid/images/imageID.png
                - data/camvid/images/labels/imageID_mask.png
                    - Write a function that retuns path to mask for that image
    - Do I need to prepare the images agian or can I use what I have gotten?
        - Have about 200 image in the Action Classfication dataset
        - 150 for training 
        - 50 for validation
    

Finish making the U-Net dataset (COMPLETE)
Train the Fastai U-Net Model
    - Downlaod the fastai colab notebook (DONE)
    - Look at the pixel codes for the mask (DONE)
        - What is inside? (DONE)
            - Unqiue values of 
                - array([0, 1, 2, 3, 4, 5], dtype=uint8)
                - [{"name":"3Point","color":"#f40930","id":2},
                    {"name":"Other","color":"#c404e5","id":0},
                    {"name":"2Point","color":"#13e9d5","id":1},
                    {"name":"Board","color":"#1fe40c","id":3},
                    {"name":"Freethrow","color":"#d3da00","id":4},
                    {"name":"Layup","color":"#1a07e7","id":5}
                  ]
    - Combine the collected data to the appropriate folder style (DONE)
    - Upload the file to Gdirve 
    - Learning how to debug NN (Notes from Andrews Ng course)
        - When do I need more data?
            - Realtionships
                - DatasetSize Proportional to Training Error---(Platoes) (Starting to learn fitting)
                - DataSize Inverse Proportional to Validation Error (Platoes) (After learning from a big dataset)
                - DataSize not related underfitting/High bias
                - DataSize Proportional to overfitting/high varience
        - Increasing or decreasing the lamda?
        - Steps:
            - Divide the data
                - training (0.7)
                - test (0.3)
                    - Compute error on test set
            - Bias vs Variance 
                - High bias/underfitting
                    - Not enough features
                - High varience/over fitting 
                    - Too good in training set but bad at validation 
                - Regularizattion lambda
                    - Too low lambdaL: High varience/overfitting
                    - Too high lamda: High bais/underfit (Dont punish the weights too much)
                - CHOOSING THE RIGHT LAMBDA!
                    - Do a lambda/penelaty test
    - Use smaller images weigths to train on bigger images
        - Canvid: orginal size= ([720,960])
    - Model is training....... 
        - Do the stage-2 (DONE)

Try predicting on a image with the unetmodel 
    -Useful links:
        - Converting Imagesegment to opencv: https://forums.fast.ai/t/converting-imagesegment-to-opencv/57838/2
    - Load the exported mdel
        - Requires you to define the accuary function acc_courtSeg
    - Prediction test
        - Predictin using the source size?
            - It returns me a image segment object and a tensor and a matrix
                - (y, pred, raw_pred)
                    - Line383 : https://github.com/fastai/fastai/blob/master/fastai/basic_train.py#L395
            - How do I use this output? (DONE)
                - how does learn.show_results work?
                    - The learner is a unet leearner, the show_results function be different from the cnnn ones
                         - ds.x.show_xyzs(xs, ys, zs, **kwargs) is called 
                            - What is xs ,ys , and zs??
                                - xs is a list of: ds.x.reconstruct(grab_idx(x, i)) object
                                    - What does reconstruct do?
                                        - transform a pytorch tensor back into an ItemBase. 
                                        - the opposite of calling ItemBase.data.
                                        - What is an ItemBase?
                                            - Something that store the tensor
                                        - What does this function return?
                                             - returns
                                                - Image(t.float().clamp(min=0,max=1)) 
                                                - ImageSegment(t)
                            - test_img.show(y=x[0], figsize=(15,15))
                                - seems to work.
                                - x[0] is what we want
                                    - x[0].data gives us the class mask (THIS IS WHAT WE NEED)
                                        - How does it color the mask?
                                            - x[0] has the same effect as openmask()
                                                - This is because openmask returns ImageSegment object
                                                    - Has save() function
                                                        -  The saved image is in [0..1,]/class mask version
                                                    - Has show() function. 
                                                        - Line 238:
                                                            -  https://github.com/fastai/fastai/blob/master/fastai/vision/image.py#L224
                                                        - Is where the class is turned colourful.
                                                        - calls the show_image function
                                                            - Line 431: https://github.com/fastai/fastai/blob/master/fastai/vision/image.py#L224
                                                            - show_image(self, ax=ax, hide_axis=hide_axis, cmap=cmap, figsize=figsize, interpolation='nearest', alpha=alpha, vmin=0, **kwargs)
                                                                - cmap: 'tab20'
                                                                - alpha: float0.5
                                                                - interpolation='nearest'
                                                                - vmin=0
                                                                -This function first converts the to something else
                                                                    - using image2np(image.data
                                                                        - converts the tensor image to numparray image
                                                                - Applying cmap is how it coverts [0...1] to coloured image
                                                        - Summary:
                                                            - openMask(file)
                                                                - openImage
                                                                    -Line 393: https://github.com/fastai/fastai/blob/master/fastai/vision/image.py#L224
                                                                    - show_image
                                                                        - What the fk is plt
                                                                        - return an AxesIamge
                                                                        - Convert the Axes image to opencv image 
                                                        - convert the image to RGBA to RGB (DONE) 
                                                            -  Turn it into BRG
                            - NEXT: READ the post from Pritesh Gohil
                                - https://stackoverflow.com/questions/28757348/how-to-clear-memory-completely-of-all-matplotlib-plots
                                    - Write a functions that reset the figure size
                                    - Write a function that applys cmap given an image mask
                            - Apply wls-filtering to import the prediciton (STOPPED CHASING)
                                - You can't because you need left and right image
                            - Try on a video that was not in the trianing or validation set
                                - (WORKING YEAH!)
                            - 





        - Producing the mask image in notebook (FAILED)
            - What is the most effiecent way to colourmap
                - Well we pass in img to show_image() it produce colour full image
                    - Look at the source how it produces the colour full image
                    - It passes it to AxexImage(self...) // slef refers to the [0..1] image
                    NEXT: I have Array of class mask. I need to apply cmap to it and get new coloured array.
                        - Try checking fastai fourm if we can save the colour map image
                            - Found nothing
                        - Using the function found in stackoverflow
                            - https://stackoverflow.com/questions/52498777/apply-matplotlib-or-custom-colormap-to-opencv-image
                            - What does scalarmappable do?
                                -  scalar data to RGBA mapping
                                    - because the are all in range [0,1,..5], they all get mapp to blue
                                    - what does scalarMappable.to_rgba do?
                                        - what does np.linspace do?
                                            - produces lineraly space numbers 
                                        - What does cv2.lut do?
        - Producing the mask image in notebook(DONE)
            - I have the img as np.array(...)
                - Can I use fastai to return me AxesImage?
                    - I can use show_image but it accetps an ImageObject. It then extract the Image data to send to matplot 
                        - I can send it the ImageSegmentOject as ImagObject but then I have to convert the data tensor to float.
                            - Does ImageSegmentOBject allow me to change the data to a new data of float?
                                -...
                - Can I just use matplotlib 
                    - What does imshow return?
                        - AxesImage
                        - Bbox image
                            - How does show_img make axesImage
                            - Next look at this: 
                                - https://stackoverflow.com/questions/13623301/convert-contour-matplotlib-or-opencv-to-image-of-the-same-size-as-the-original
                                    - Get the image with out plotting it the image
                                        - How does im_plot.make_image know which image I am talking about?
                                            - I am telling it in plt.imshow. And plt.imshow must returns image.axesImage    
                                                - How does plt.imshow make the axesImage?
                                                    - It calls gca().imshow()
                                                        - What is gca()?
                                                            - returns an axiesimage object
                                                            - the imshow() function of axesimage object is returned
                                                                - Does that mean axesimage imshow function accepts array?


                                                       - How does gca return me the axes image?
        - Make a fuction that takes mask and returns the usable image              


Team detection
    - I need to a model that does human detection
        - Human detection Now working
    -Filtering the Human detection, by box ratio, area then filtering these people for team (DONE)
        - Only draw the prediction if the following conditions satifies
            - Box size
                - Find the area of the box
                    - What does box from prediction contain=[left , top, width ,height]
                        - what does each element stand for?
                            - top left coord : (left,top)
                            - bottom right coord: (left + width, top + height)
                        - Yeah, you can just do width * height
                - Find the area of the image 
                - boxPercentage: aereaBox/area_img
                    - <=0.5
            - Label
                - DONE
        - I want it suh that at any given time (HOLD) 
            - The number of detection is 11 
    ROI color extraction
        - Extract ROI
            - Crop the image 
                - easy list slicing
        - Process the ROI 
            - Find the team section
                - max_ratio
                    - We only know the for sure the team color if 
                    - max ratio is greater than some threshold
                    - How do we get max_ratio?
                        - its the maximum value of the ratioList
                        - What is the ratio list?
                            - Its an empty list that gets filled 
                            - What is it filled it?
                                - it is filled with ratio
                                - ratioList = [fractionOfPixelsInColourClass1 , fractionOfPixelsInColoutClass2]
                                - What is the ratio? (DONE)
                                    - color_pix/tot_pix
                                        - numberofpxilesthatfitthecolorcertia/totoalnumberofpixles
                                        - In simple terms: find the ratio of pixels that find into your 2 colour classes
                                    - what the fk is color_pix?(DONE) (RETURN HERE)
                                        - count_nonblack_np(output)
                                            - in a cropped region of a white player:
                                                - There still exist black pixels 
                                                    - If the pixels are black that means we threw them away
                                                - Simple terms: 
                                                    - it returns number of pixles that falls in the boundary
                                        - number of non-black pixels in the image
                                            - returns the number of pixles that were in the selection
                                            - 
                                        - Are we still talking about cropped image?
                                            -NO we are talking about:
                                                - cv2.bitwise_and(image, image, mask = mask)
                                                    - What does bitwise_and do?
                                                        - mask contains YES region. (YES region = white area)
                                                        - bitwise_and: in a given image keeps the YES regions only
                                                    - what is the image?
                                                        - this is the cropped image
                                                    - What is the mask?
                                                        - cv2.inRange(image, lower, upper)
                                                        - What does cv2.inRange(image, lower, higher) do?
                                                            - thresholding based on range of pixels instead of 1
                                                            - Because hue tell use what color its with just scalar value. We use H thus HSV
                                                            - How does she get the threshold? (DONE)
                                                                - For each color there is a threshold
                                                                    - eg. color: (lower_bound , upper_bound)
                                                                    - white: ([0,0,200], [255,255,255])
                                                                    - black: ([0, 0, 0], [255, 255, 60]) 
                                                                    - Where do these numbers come from?
                                                                        - What do these numbers mean?
                                                                            - [lowHue,loewSaturation,lowValue] 
                                                                            - [highHue,......]
                                                                        - Why is the lower bound for white 0?
                                                                        -Experiment
                                                                            - [0,0,0] , [0,0,0]
                                                                                - All black
                                                                            -[255,255,255] , [255,255,255]
                                                                                - only the whitest area is YESED
                                                                            - [0,0,0] , [255,255,255]
                                                                                - All blacked
                                                                                - WHY?
                                                                                    - 
                                                                            - [1,1,1] , [255,255,255]
                                                                                - All yesed
                                                                            -  mask o yellow (15,0,0) ~ (36, 255, 255)
                                                                                - Is it yellow:
                                                                                    -(36,...) or (...,36)
                                                                                    - ITs: (36, 255, 255) ~(BGR)
                                                                                -White
                                                                                    - [0,0,200], [255,255,255]
                                                                            - H: 0-179, S: 0-255, V: 0-255.
                                                                    - The followin link teaches use how to set the boundaries in HSV
                                                                        - https://stackoverflow.com/questions/10948589/choosing-the-correct-upper-and-lower-hsv-boundaries-for-color-detection-withcv
                                                                        -User: Knight

                    - When do we turn the image into HSV
                        - The image we pass into inRange has to be HSV
                            - The arguments that 
NEXT: Finish reading detect_team function at.. and finish the "What is the mask thread"
    - https://github.com/priya-dwivedi/Deep-Learning/blob/master/march_madness_team_shot/object_detection/March_Madness_Object_Detection.ipynb
    -https://docs.opencv.org/3.4/da/d97/tutorial_threshold_inRange.html



Ball Regression
    - Trying seeing if you have turned convert the image to RGB before you are passing it to the detector?
    - The image regresion model is up right bad. Because the dataset that I trainon was really bad
        - Remake the dataset.
            - What format should the test and validation set be in?
                - The file name are the coords
                - eg. 9,10.png
            - Find image point labeler tool 
                - Predict 2 points
                    - The shooting player
                    - The ball 
                    - In THAT order!
        - Train the model
            - I have to rename each file 
                - humanx, humany , ballx, bally.png
                    - exatract the information out of the xml files 
                    - 

Debug:
    - CUDA out of memory. Tried to allocate 24.72 GiB (GPU 0; 7.43 GiB total capacity;(SOLVED)
        - Resize/crop/pad the training image set to fit the standard sized used in the pretrained model you are using. Ex: 224x224 for Resnet, 299x299 Inception etc
        - Look at how to resize the dataset
            - Fix: The accuracy function was not great

    - figureImage.make_image() does not work (SLOVED) 
        - say: 'str' object has no attribute 'dpi'
            - fac = renderer.dpi/self.figure.dpi
            - It make sense, because renderer is a string
            - How does axesimage makes its renderer
                - It does not use the rendere varaible
                - What is a rendere? It not a dictionary because we cant do rendere.dpi 
                    - My guess is it is some kind of object
                        - Made an renderer object and passed it down






