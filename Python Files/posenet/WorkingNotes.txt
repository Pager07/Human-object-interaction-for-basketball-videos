Proejct Goals:
    - Ball detection (DONE)
        - Find out why the predictions so shit?
        - Does it differ If I create my own dataset?
    - Action Classification (DONE)
        - Find out why the prediction are so shit?
        -Does it differ If I create my own dataset?
    - Player Detection (DONE)
    - Start on team seperation (DONE) 
    - Court and board Detection 
===================================================================================================================================================
===================================================================================================================================================
===================================================================================================================================================
May Experiment with these in future:
- https://towardsdatascience.com/detecting-soccer-palyers-and-ball-retinantet-2ab5f997ab2


Project 
    - What other methods are avialable to detect ball??
        - Resenet50 CNN regression?
            - What does the dataset format look like? (Image and points)
                - A points file txt file
                    - With same filename
                    
            - What is a datapoint going to be?
            - Are there any such dataset ot there?
                -http://ipl.ce.sharif.edu/ball_datasets.html
                - http://basketballattention.appspot.com/
            - How is the process of collecting the dataset?

Download videos
    - Take screen shots 
    - Label them


- I am trying to test CAT regression on a single image 
    -  export the export.pkl file 
    -  It contains the following
        - (the model, the weights but also some metadata like the classes or the transforms/normalization used).
    - Yes, Restnet is able to predict where ears and nose even in occlusion.

STEPS:
        -  Create the learner
        -  Load the weights
        - Export the learner


Creating Basketball DataSet
    - Get frames at given timstamps
        - x1000 to get timestamp in mircoseconds
        - Convert it to seconds
        - https://www.youtube.com/watch?time_continue=SECONDS!!!!!&v=VIDEOIDDDD
            -example: https://www.youtube.com/watch?time_continue=2805&v=AALM9_RgV6c
        - Is there a way I can extract out the frame form youtube videos?
    - Given an Screen shot I know where the ball is
    - Get screen shots
        - Get a screen shot using code (DONE)
            - Login to youtube 
            - Use the page to search for a vidoeo's screen shot 
        -Get screen shots of the sent urls 
            - resize the image in JS
            - Redirect the chrome downloaded item to a dataset folder
            - Build a function that send a JSON STRING
                - REQUEST STRING:
                        - '{links:{0:'link1',1:link2.....}}
                        - '{links:'link1,line2.....'}' (USING THIS)
                    - What is the best way to send this data
                        - We need to use a middleeware to be able recieve data outside params 
                            - var bodyParser = require('body-parser');
                            - app.use(bodyParser.urlencoded({ extended: false }))
                            - data will be available on req.body.links
                            - app.post() will take aynsc function
                - JSONIFY THEM 
                - PROCCESS EACH LINK 
                - SEND THE IMAGE
                - REAPEAT UNTIL ALL THE LINKS HAVE BEEN PPROCCESSD!


Training Images I am not sure about
    - 164,31.png
        - I can get the path of an Image
            - data.train_ds.x.items[idx]
        - How to gwt the idx?
        - Lets try to learn anyyway, 
            - Itf the result is really bad, we will go and fix the lable using opencv 

Task 
 - Train the model in larger images size(Done)
    - Acutal data size: 490x360(Done)
    - Can I get any sort of x and y coords form the fast ai
        - Maybe look up how image.show(y=ImagePoints), takes imagespoints and finds the x y coord to find the dor
            - Imagepoints: Potential target to be superposed on the same graph (mask, bounding box, points) 
                - So its like a sketch paper on top of an image, its is the same size as the image
                    - So may be find what coord holds the red dot in the image points?
                        - Inside image points, Coords are scaled to range (-1,1), iff SCALE== true
                            - Find out how it scales the coords and reserve it by tracking what happens SCALE==TRUE?
                                - scale_flow() function gets ran
                                    -https://github.com/fastai/fastai/blob/master/fastai/vision/image.py#L251 LINE:440 (NEXT)
                                    - to_unit = True when scale_flow is true
                                        - flow.flow = flow.flow/s-1
                                            - I guess flow.flow are the coordinates
                                             - s = flow.size[0]/2 , flow.size[1]/2
                                             - What does the flow size look like relative to the image? Is it shape of the image?
                                                - imagePoints.size return flows size, which are (360,490)
                                                - s seems to be the center coord of the image

- Run fastai in my pc (DONE)
    -https://medium.com/@plape/how-to-install-fastai-on-mac-a05496670926


- Mapping the coords given by the model for image size (490,360) to orginial image coord (DONE)
    - https://stackoverflow.com/questions/25684327/how-to-find-an-equivalent-point-in-a-scaled-down-image
    - Draw a cricle around the original image
        - coordGiven: y,x
        - originalImage: x',y'
            - Find scaleFactor: sizex'/sizex..
                - NOTE: opencv: (width, height)
        - NewCood: x *  scaleFactor
    -NEXT: COMPLETE THIS!



The cricle is not drawn in the right part oof the image
    - Even when I cchange the center, the cricle is not chaingein it place
    - The originalCoord is wrong
    - Check if the given unscaledCoord is correct
        - Try drawing a circle in a random image
        - Its wrong: Becuase I forgot to divide s coord by /2


I getting Confused with W and H 
    - image.size = (360,490) = (H,W)
    - image.data = [-0.46 , -0.25] = (H,W)
    OPENCV
    - img.shape = [360,490]  = (H,W)
    - cv2.resize(img , (490,360)) = (W,H)
    - cv2.cricle(img, (490,360)) = (W,H)]
    - img[row ,col] : [H,W]
I have an array of image -> FastAi Image Object (NEXT)

Detection on a video
    - Pause 

Progees: 
    - I can take. picture and do regression on the image to the location of the ball
NEXT: 
    - However, the predictions of the ball is not quite accurate
    - I will try fixiing the labels 
    - Train it with more images?

Ball detection
--------
Action Classification
- Classification model
    - I will trey this 1st
- Posenet
    - The idea is to get latent features from pose net and do action detection 

Classification model
    - I need to make a dataset
        - I need labels, look at the csv file
            - The second last column has the classes
                - 3-pointer success(DONE) (CHECKED)(CHECKED)
                    - train:348(need more)(880)
                    - val: (DONE)
                -3-pointer failure, (DONE)(CHECKED)(CHECKED)
                    - train:701
                    - val: (DONE)

                -free-throw success,(DONE)(CHECKED)(CHECKED)
                    - train:221 (NEED MORE)(542)
                    - val: (DONE)
                - free-throw failure, (DONE)(CHECKED)(CHECKED)
                    - train:135 (NEED MORE)(339 total)
                    - val: (DONE)


                - layup success, (DONE) (CHECKED) (CHECKED)
                    - train: 408 (NEED MORE)(1189 total)
                    - val:(DONE)
                - layup failure,(DONE)(CHECKED)(CHECKEd)
                    - train: 517
                    - val:(DONE)
                - other 2-pointer success, (DONE)(CHECKED)(CHECKEd)
                    - train:409 (NEED MORE)(1017 total)
                    - val: (DONE)
                - other 2-pointer failure, (DONE)(CHECKED) (CHECKED)
                    - train:726 (DONE)
                    - val: (DONE)
                - slam dunk success: (DONE) (CHECKED)(CHECKED)(might need a bit more valid more on validation set)
                    - train:108 (NEED MORE) (279)total
                    - val: (DONE)
                - slam dunk failure: (DONE)
                    -train: 18 (NEED MORE)(46 total)

        - total Train : 
        - toal test : 
    - The video links are going to be same but the label links has to change
        - Check how I get ball possition 
        - I can just access it using row['actionSuccess']
         - if I just name them with the classname, the image file is going to replace each other
            - need some kind of unique
                - index_actionClass.png: 1_shoot.png
                     - The index is relative to the main big table not the cut out training and val table
                        - Downloading training images into: /Volumes/My Passport/FinalYearProjectData/ActionClassification/train
                        - Got an error in server when downloading for this link https://www.youtube.com/watch?v=cr3caxj-4E4&t=46m17s
They way I downloaded the dataset is not helpful
    - I need to be able to see how many image each classes have 
    - I need to download 100 more image extra just incase I remove due to falulty images

Reorganise the data in a good format
    - Take a look I orginsed the data for other classification 
        - Inside the data folder
            - I have classes folder (DONE)

TODO:
    - I will concatonate slam dunk success and failure class
    - There are alot of flase labels
    - There are 2 options
        - Download the data set for each class 1 by 1 agian 
        - Download only from the point of last image 
            - Make a new dataframe from oldlocation to new loaction 
                - Download thoes images
                -example: train_links_df[train_links_df['actionSuccess'] == 'slam dunk success'].loc[8852:]
        - can I fix timing issue?
            - what is causing the timing error?
                - I am lookin at 5 dummy links for time lag
                    - first image, timing is correct, its taken at the end of the second 
                    - second image, timing is wrong, given:40 and but taken at 42 (2 second later)
                    - third image, timingi is wrong, given:6 but taken at early 8 (2 second later)
                    - fourth image, timing is wrong, given:7 ubt takeb at early 8 (1 seond later )
                    - fifth imag, timing is wrong, given 46 take at 48 (1 second later)
                - Its seems that for a most links it is taken after 2 seconds
                - Is there a to get more specific timings?
                    - Try: pausing the video?
                        - pausing the video gets better results, 
                        - next: I might have to wait a bit longer so that the pause mark goes away
                    -Try: I am trying to if in get_time if I choose between option of round down or up
                        -currently i have -1, which tell me I it is giveing me 1 second back
                            - What happenns if I remove this 1 second, and give better rounded sec
                                - just round: lead to bad reuslt
                                - round - 1 : bad results
                                - round - 2 : better results
                                    - When was the actually take comppared to the orginiaail 
                                        - Because we are using round, it is much closer to original time
                                            - 54:53
                                            - 42:41
                                            - 8:7( 1 second delay is good because @8 player has already ladnded)

                                - round - 3 : video has go to much backward in time 
                        - Hence, I will just stick to round()-2

                    - Each data is going propely labled with the following info
                        - ballPos
                        -idnex
                        -name
    - Cleaning up data from each class (DONE)
        - Remove image that clearly should not be there
    - Training the data 

    - Some classes look very similar, what soloution is there?

------------
MERGing Classification model and regression model together for demo
    - Dont forget to convert shit to rgb 
        -test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)
    Regression Model Class:
        - getPrediction()
        - drawPrediction()
    Action classification
        - getPrediction()
    - Need to load the 2 notebooks
    -Side note: Classes are in this order
       - ['3-pointer failure',
          '3-pointer success',
          'free-throw failure',
          'free-throw success',
          'layup failure',
          'layup success',
          'other 2-pointer failure',
          'other 2-pointer success',
          'slam dunk']
-------------


Detecting Teams
    - What does mask of specific colours mean?
        - Masking means: 
            - Cutting  out only the portion of image that you need and placing it an empty black image template
            - Hence, the resulting image will be same as the image, in a black background
            - Just like Masking in Final Cut Pro
    - Does detecting teams depend on detecting player first?
        -  we extracted bounding box from tensorflow object detection 
        -  count the percent of pixels in that bounding box that are non black to decide the team for that player.
        - 


---------
 Player Detection
    - What is the best model to do player detection?
        - Measurement they use is mAP
            - Naming Convention
                - Negatives
                    -  Wrong Y 
                - Positives
                    - Acutal Y
                - True
                    - Correctly Classifying
                    - Arrow is going the correct direction
                - Flase
                    - Incorrectly Classfying
                    - Arrow is going the wrong direction
            - precision 
                - if it does predict y as Y: how likely is it that its the top arrow (accurary of top arroow)
            - recall
                - probabilty that it find the (y=y) how good you find all the positives.
            - IOU
-----------------------------
REDO 

Ball Regression and Action classification
    - Create 1 dataset that serves both 
        - Collect yourself 
            - Downlaod images 
                - 2 Points
                - Layup 
                - Dunk
        - Search  
            - Google?
            - Github?

Questions:
    - What did the Yolo-Basketball detection dataset look like?
-------
Presentation:
    - 10 minutes
    - Title slide
        - Project title, name, course, data 
        - Outline
    - Problem description 
    - Motivation
    - Approach
    - What you have commplished so far(Not mars givens)
    - Analyisis (how will you judge the outcome of your work)
    -A conlusion slide (what did or will you accomplish)
    - What still needs to be done?
    -Clarity of explanation of the deliverables 
    - How will the project be evaluated?


Possibile themes 
    - https://slidesgo.com/theme/data-waves
    - https://slidesgo.com/theme/ai-tech-agency
    -https://slidemodel.com/templates/basketball-court-game-plan-powerpoint-shapes/


QUESTIONS?
    - WHAT IS HOI?
        -The Human-object interaction (HOI) detection is the task of localizing and inferring relationships between a human and an object.
        -  For example, detecting the HOI “human-row-boat” refers to localizing a “human,” a “boat,” and predicting the interaction “row” for this human-object pair. 
    - Why I choose HOI on basketball
        Algorithmic advantages of doing HOI in basketball
        - The number of objects in the object space is limited
        - The action is some what related 

SCRPIT
    laaning an Preperation 
    - Preminalry preperation: instead of for fun.
    - Over the christmas break in my second year, I did my very first  deeping learning project every. More like a helloWorld project
        - It was about cats vs dogs recognition    
        - Here is me, telling every one in the world how I did the projct and just telling people the how happy on making a computer recognise cats and dogs
            - I was expecting people to be like: oh it took you a week to do this piece of cake, it takes me 30 mins.
            - But nope: responses that I received was very encouraging
    - So with newly. gained confidence, I moved on and  did the same thing over different dataset over and over aigan with minor varitations,
        - Some of the cool dataset that I got my hands dirty with is the minist and cats facial feature 
        - Minist was cool becauuse It was a multi-classification problem meaning there were more than 2 classes to predict from.
        - Cats facial feature prediction is fav ...
            - It is a regression problem, it where you take an image and have the superpower to predict a continous value 
            - The model was doing great.
            - But I was like what happens, if I block of some of the features,forexample, if I remove the cats eye  will it still be able to predict where the eye is??
                - So here are some results. When I saw this, I was like what NO WAY. It's so dope, its like if the the model is reallly good at throwing a dart with half blind. 
    - So Why am I telling you all these? Its because, these are the preliminary project and experiements for my 3rd year project. And form the back of my main 3rd project. 

    Motivation
        - At this points, I am getting bored. Cause, all I am constally doing interperting an image via object bounding boxes and Nothing else.
        - Wouldn't it sick though, if your model could understand what's going on in a image. Perhasp, extract semantic information out of the image.   
        - You know after a basketball game, players have to sit hours of flim session to review the game with a tv remote controller. I mean remote controller. At this day an age you are tidously using a remote controller . Hold up I got something better for what if I give you a system where you type in layup and it retrieves you all the clips of you laying up. 
        - Focusing creative engery 
        - There are other advantages  camera planning, image captioning. 
        - If solution is to exist, it can def improve enjoymnet of the sport. 


    Problem Description
    - So the umbrella term for my project is called Human-Object interaction recognition detection via Machine learning 
        - I know it a mouthful but in laymans terms: All it means is just find where the human is, find where the object of interest is. And assing them a verb. Therefore, the goal is to represent it  human object interaction as a triplet (SLides eefect draw boiund boxes)
        - This problem is hard because, you will have an exponentail expolsion here.
        - So for this project, I am trying to slove HOI in the basketball domain. The size of the object and human space is relativley relaxed.
        - The object space only contains 1 object and that is the ball
        - Well for the human space, we dont have consider the spectators, cameramans , ref so The human space is shirnked down into only the players
        - So that a thumbs up, as far as my time and  computational resources required
            - Meaning less dataset required
            - Lower model training time
            - Which means , I have more time to improve and compare model with others and do alot of other experiments
    - However, HOI with basketball has it own unqie set of challanges 
        - I extermely hard to localise the basketball. 
            - First, believe me when I say the ball appears really small and looks like an organe dot and just uses too few pixles. 
            - Plus, when it moving, I not even a dot anymore,  so it appears blurred out. And if the player is moving with it too fast, it dissapears. 
            - Sometime, it dissapears because it is occluded by players
        - Player interact with object in various ways that look very similar. 

    Apporach
        - So How do we tackel this problem?
        - So lets bring back the computer scientist mentality here: Lets break the problem down to smaller subproblem. And lets slove the subproblem. Which will inturn slove the main problem
        - The sub problems are 
            - Localizing  the player
            - Localizing the ball
            - Classifiing the verb
        - Well
            - Localizing  the player
                - Its th reasy part. Use pre-trained  model like YOLO to detect humans and filter the detecting them using  computer vision teq like thresholding
            - Localizing the ball
                -  Lets use a regression model, to interpolate the spatail coords just like we did with cat facail featuers 
            - Classifying the verb
                - Well we can use multi-classification model just like we did with minsit. Where 1 action is a 1 class  
    Deiliverabls
        - So that when I have done all the stuff, I want to be able to take a video and analyze frames and report on what the player is doing with the ball
        - And the I want to go the extra mile, detecting teams to produce action stats sheet for each team. So say like, Lakers only have taken  3 pointer, the def need to .
    Evaluation
        - Quantative Evaluation will include, evaluating confusion matrix 
            - This is  the confusion matrix, of my model that classify verb. As you can see there 
            - At this point there are alots of false postives and flase negatives
                - between classes that look similar such as training  
        - Then I will be evaluating its Percsion and Recall Scores
            - to find out how accurate the prediction are and how often model detects a certain action
        - Becuase percsion and recall are both inportant, I will be evaluting the models  F1 score as well.  

            
---------
New ideas:
    - Time Compression
        - Assumes that the camera is stationary?
        -  That would certainly create multiple, and cumulative instances of the same subject on the same frame. 
            -Since I cannot turn on and off the sun, I am going to use a light source that can be controlled in that fashion: Canon Speedlites will offer that capability very nicely.
            - Exposure data is 0.5 sec at f-8. During that half-second exposure, the flash that was mounted on the hot shoe of the camera, a Canon G9, fired multiple times to register the number of images you see here.
    - What happens if I add mutliples frames in to 1
        - Experiment:
            - Combining All the frames in the clip
                - mergedImage = w * oldImage  + w2 * newImage
                    - Increasing w2 brigthens up 
How to draw key points?(Complete)
    - https://github.com/luxifeo/posenet-python/blob/master/posenet/utils.py
    - uses 
        -out_img = cv2.drawKeypoints(img, cv_keypoints, outImage=np.array([]))
        - what is cv_key_points? (DONE)
            - Its is list of something
                - something: cv2.keyPoints object
                    -cv2.KeyPoint(kc[1], kc[0], 10. * ks)
                -What is kc[1] and kc[0]?
                    -kc = key_coords.
                    -x= kc[0] and y=kc[1] or the reverse
                - Note: Every point comes with
                    - (score, part, position)
                - what is ii?
                    - index of score
                - what is ks?
            - This function seems to give out empty image, Who calls this function? Do they fill it up latee
How do I draw skeleton? (DONE)
    - What does, get_adjacent_keypoints(keypoint_scores[ii, :], keypoint_coords[ii, :, :], min_part_confidence) do?
        - What is posenet.CONNECTED_PART_INDICES?
            - imported from posnet.constants
            - What is PART_IDS?
                - {pn: pid for pid, pn in enumerate(PART_NAMES)}

        - What is results?
            - Its an array of array. [ [leftPart , rightPart],....otherpairs]
            - pair = np.array([keypoint_coords[left][::-1], keypoint_coords[right][::-1]]).astype(np.int32)
            - What does [::-1] do? Its reverses lefpart: (x,y ) -> (y,x)
            - What is left and right? 
                - Its an index of where of where left and right respective keypoint is  found
        - For 1 pose/keypoint/{part:leftLeg} . it find the counter part rightLeg coord and return a [{x:,y:} , {x:,y}]
        - What does  keypoint_coords[ii, :, :] do? 

How do draw keypoints and skeleton? (Complete)

How to convert array of images to base64? (Complete)
    - https://stackoverflow.com/questions/16065694/is-it-possible-to-create-encoded-base64-url-from-image-object
    - https://stackoverflow.com/questions/40928205/python-opencv-image-to-byte-string-for-json-transfer (WORKED)
Learn about heatmaps? (Complete)
    - Read abit 
Read about the mouse project?(Complete)
    - Encode mouse movements, directions, speed and acceleration of mouse in a consistent way
        - Allowed to encode trajactory lines
- Produce an image containing all the poses off all the frames in an image (COMPLETE)
    - On a black image (DONE)
    - On a non black image(DONE) 
        - 1 second --> 60 frame

- Produce an image containg all the poses of all the frames in an image and color code it (Complete)
    - How to map numberOfFrame with colour
        - Learn the different colour spaces avialable?
            - What is a colour space?
                - three-dimensional object which contains all realizable color combinations.
            - What are the different colour spaces avialable?
                - HSV
                    - (hue, saturation, brightness)
                        - hue: which colour(expressed as a number from 0 to 360)
                        - saturation: pruity of the choosen choosen colour (toward zero introduces more gray and produces a faded effect , less pure more gray)
                        - value:  brightness or intensity
    - Example of colou mapping
        - learnopencv.com/applycolormap-for-pseudocoloring-in-opencv-c-python/
            - What they are doing is mapping one colour to another colour
                - turns it into an grayImage 
                - uses applyColorMap() to map the image to anthoer color 
            - NEXT:::What I want is map a frame count to another colour
                - Then use this colour to draw the poses
                - The problem: Mapping interger into RGB/HSV space?
                    -  Does cv2.polylines accept 

How to map an integer in range 0-60 to a color spectrum?  (Complete)
    - Steps
        - Get the perctange value (Normalize)
         - Then the number is mapped to a color using a subclass of colormap
            - LinearSCM
            - ListedColorMap
    - Tutorial:
        - https://matplotlib.org/tutorials/colors/colormap-manipulation.html
    - Returns RBGA 
        - what does cv2.polylines and drawKeyPoints accept?
            - RGB or BRG
            - Do they accept fractions?
                - BRG
                - Does not accept fractions has to be in the range 0-255
Debug
    - It is only using yellow color
        - (90, 7, 7) then (37, 231, 231) and the last color is just repeated
    - mapINTocoolours is geting negative numbers (totalColors)
        - totalFrames is beign pass as totalColours
            -Why is totalNumberFrames so big?
                - -230353946974395
                - videoPath: ._Free-throw39.mov

Try to play around with only darwing fewr frame and see if that make a differet (COMPLETE)
    - There are 58 frames => 2 seconds 
    - 29 frames => 1 second
    - Tried:
        - Every 10 frames (Time need)
        - Every 5 frames (Time)
        - Every 2 frames (Time) 
 - Collect dataset (COMPLETE)
    50%-30% split
    50 Train 
    15 Valid
    - Test classes:
        - 3-Pointer (DONE)
        - Free-throw 
        - Layup
    -Download the videos 
        - There are 2 way
            - I can 1 long video with many 2 seconds 3-pointers video
                - May have problems
            - I can make 3-pointer video clips direclty
                - Try this
            - Linked Used
                -Free-throw
                    - https://www.youtube.com/watch?v=7tW3oYk43yg
                    - https://www.youtube.com/watch?v=qawlTEmBEns
                        -NBA Airball Free Throws Compilation

- Next:Train the on new images 
- Can I label the hand differnt color?
- I notice there are some consistency in the basketball court 
    - The drawing on the ground is always the same
    - The inner box has one dominiant color
    - There is always the stateframe writing in the rim
- May be I can uses statiatical patterns of feature poins to recognise categgories of objects?
    - May be I can do feature points macting 
        - Do SIFT Feature Matching
            - Given a feature in l1 (STATE FRAME),how to find the best match in l2?
                - I am gonna go make some notes on SIFT Feature Matching now

What does surf dectection.py contain?
    - line 234 has drawKeyPoints
        - At this stage we must have done the processing already
        - feature_object.detectAndCompute()
            - What is feature_object?(DONE)
                - it is created by
                    -  cv2.xfeatures2d.SIFT_create(400) or by 
                        - 400 is the hessiabThreshold
                        - Wtf is hessian threshold?
                    - cv2.ORB_create(800)
                - It is an Feature2D object class, that extracts features
                - What does .detectAndCompute()
                    - takes image/crop and mask
                        - What is crop?
                            - basically the region with feature that you want to detect
                    - returns keypoints_cropped_region and descriptors_cropped_region
                        - I understand what keypoints are
                            - It tell you  where the feature is located at
                        - What is a decriptors_cropped_region?
                            - decriptors are the actually feature values
    - Thee feature mapping is happening in Feature Matches window
        - Maybe I should look there?
        - it is using cv2.drawMatches(crop,keypoints_cropped_region,frame,keypoints,good_matches,None,**draw_params)
            - crop(objectImage) , keypoints_cropped_region(featurePoints in the main object image)
            - frame(current Frame), keyspoints (I think it find feature points just like crop and set this var)
            - good_match(matches1to2)
                -DMatch object: Object for matching keypoints
        - Is keypoints gernerated how I think it is? 
            - Yes: a gray_frame is passed
        - How is good_matches created? (DONE)
            - it is a list of tuples (m(features/descriptors in objectImage) , n (features/descriptors in training image)):
                - we are filtering tuples(m,n) in matches list 
                    - based on: m.distance < 0.7*n.distance
                - How is matches list created?
                    - matcher.knnMatch(descriptors_cropped_region, trainDescriptors = descriptors, k = 2)
                        - Returns you (m,n) which are matching features (matchobject)
                    - What is a matcher?(DONE)
                        - is returned from cv2.FlannBasedMatcher(index_params,search_params) or cv2.BFMatcher()(Brute Force Matcher) depending on cv2 version
                            - The returned object is a DescriptorMatcher class
                            - (Class for matching keypoints descriptors)

                        -  The frist one only works for OpenCv > 3.1
                            - We have 3.4.5! We can good to go!
                        - WTF is index_params and search_params?
                            -  index_params: 
                                - KDTREEPARAM in the form of dictionary: 
                                    -dict(algorithm = FLANN_INDEX_KDTREE, trees = 1)
                            - search_param
                                - Number of searches maybe?
                                    - another dict:  dict(checks=50)
                    - Finally matches between the 2 features Steps
                        - matcher.knnMatch(descriptors_cropped_region, trainDescriptors = descriptors, k = 2) 
                            - trainDescriptors = descriptors: is the descriptors/features from the actually image
        - Wtf is findHomography? (DONE)
            - homography: matrix transform
                - Find matrix transfromation from 1 set to another using RANSAC
                    - HUH? WHAT DOES RANSAC DO?
                    - If you want to map the objectImage to the frame you got 
            - cv2.findHomography(source_pts, destination_pts, cv2.RANSAC, 5.0)
                - What are source_pts?
                    -array: np.float32([ keypoints_cropped_region[m.queryIdx].pt for m in good_matches ]).reshape(-1,1,2)
                    - wtf is queryIdx?
                        - it bascially store the index of feature in the orignial list 
                    - keypoint.pt gives a point object (coords of the key points)
                        - I can access x and y by pt.x , pt.y
                    - what does .reshape(-1,1,2) do?    
                        - Fit the data in (1,2) and you can have as many rows as you want
        - How does he get the statistical fit?
            - Get the points objects in the frame
                - np.float32([ keypoints[m.trainIdx].pt for m in good_matches ]).reshape(-1,1,2)
            - what does cv2.fitEllipse() do?
                - find the best Ellipse for the object
                - Uses this to draw the ellipse on the frame
                    - cv2.ellipse(frame, ellipseFit, (0, 0, 255), 2, 8)
    - How does he draw matches?
        - what does cv2.drawMatches()? 
            - returns an image


STEPS:
    - Draw surf/sift keypoint on an image (
        - we need xfeature2d
         - Could not install it 
        - using external github project
            - Takes to much time 
                - img (30sec)
                    - 195*164
                - img2 (5 mins)
                    -1000x700 (aboutx)

    - Draw orb keypoints on an image (DONE)
    - Do feature matching in 2 image (DONE)
        - Not detecting feature properly

GOAL: DETECTING COURT LINES AND BOARD:
     Do canny edge and then feature matching?

    Try good featuers to tack?
    Do canny on a single image(Done)
    Filter edges in canny
        - Did really bad
    Look into Hough Lines Detection
        - The following paper detects the main court lines 
            -https://web.stanford.edu/class/ee368/Project_Spring_1415/Reports/Cheshire_Halasz_Perin.pdf 
            - Change colour space (Done)
            - Convert to H-Space (Done)
            - Create binary model
                - Alot of pixel values are being set to 0 
                - They dont pass the threshold value
                    - setting threshVal = 40 works
            - Perform erosion and dialiaton(DONE)
            - Perform Canny edge (DONE)
            - Hough transform in order to detect the line
        - THe following paper is doing what I want
            - https://www.cs.ubc.ca/~murphyk/Papers/weilwun-pami12.pdf
                - They match the court model with the edge of the video frames to establish correpondence
                    - What is a court model?
                        - - What does it mean to match edge of the vide with court model?
                            - The edge of the video frames are computed using Canny detector
                                - You have to specify firt frame homography? How do I get this?
                    - What does establishing correspondence mean?
        - The following paper is doing court detection has code for it 
            -https://github.com/stephanj/basketballVideoAnalysis/blob/master/court-detection
            - houghlines methods just does not work too well
            - Trying the autoencoder way
                - What do I have to build and perpare for this to work?
                    - WTF is a regularised autoencoder?
                        - Main idea: Regularize the rescontruction of the autoencoder by introducing more stuff in its loss fuction 
                            - Basically, controll what it reconstructs
                                - Example, denosing stuff. We are controlling what it reconstructs. 
                                - We dont want it to just learn to replicate



    - Detection by dominant colour
        - Court colour is the most domninant color (DONE)
        - The change in the color 


Understand basic of features 
    - Feature unqinuess
        - High change in in all direction
    - Types
        - Harris feature points 
            - Does really well with Rotation and translation
            - Bad with scale
        - SIFT 
            - Good with scale
            - Hesassian matrix is talke about when Feature Point Filtering is mention
                - H: Something to do with eigenValues and eigenvectors
        - SIFT feature matching
            - We need to compare features between 2 images 
                - Uses k-D trees 

The court Detection and the board detection can be solved using Imagesegmentation model
    - Why would this solve the problem?
        - The segment can tell us where the board is 
        - Each section of the court etc 
    - What kind of dataset do I need to build?
         - x: image
         - y: imagemask
            - Where pixles turn to unqiue integers for class.
            - Dont conufuse your self. fastai open mask will color code it for you 
        - codes.txt
            - where index of the item refer to the specific item 
            - exmaple. [food, water , pc , ...]
                - in your mask your should have pc pixels labeled as 3

    - What tool do I need to make the data set?
        - How to install the tool?
            - Try pixelAnnotationTool : https://github.com/abreheret/PixelAnnotationTool/releases
                - Cant download 
            - Try: https://image.predicted.ai/app
                - Can not download the images/ Works in icognito
                - Files to segment
                    - Train
                        - 2-Pointer 
                            - #13e9d5
                        - 3-Pointer
                            - #f40930
                            - TrainFolder:(DONE)
                            - ValFolder: Dpme
                        - Dunk
                            - Train:(DONE)
                            - Valid: (DONE)
                        - Layup
                            - #1a07e7
                            - (DONE)
                        -Other
                            - #c404e5
                        - Board 
                            - #1fe40c
                        - Freethrow
                            - #d3da00
                    - Valid 
                        - 3 Pointer
                        - Dunk
                - All the lables file go to Labels folder
        - What folder setting do I need for the data?
            - This the set for camvid from youtube video
                - https://www.youtube.com/watch?v=MpZxV6DVsmM&t=193s: @1:03:41
                - data/camvid/images/imageID.png
                - data/camvid/images/labels/imageID_mask.png
                    - Write a function that retuns path to mask for that image
    - Do I need to prepare the images agian or can I use what I have gotten?
        - Have about 200 image in the Action Classfication dataset
        - 150 for training 
        - 50 for validation
    

Finish making the U-Net dataset (COMPLETE)
Train the Fastai U-Net Model
    - Downlaod the fastai colab notebook (DONE)
    - Look at the pixel codes for the mask (DONE)
        - What is inside? (DONE)
            - Unqiue values of 
                - array([0, 1, 2, 3, 4, 5], dtype=uint8)
                - [{"name":"3Point","color":"#f40930","id":2},
                    {"name":"Other","color":"#c404e5","id":0},
                    {"name":"2Point","color":"#13e9d5","id":1},
                    {"name":"Board","color":"#1fe40c","id":3},
                    {"name":"Freethrow","color":"#d3da00","id":4},
                    {"name":"Layup","color":"#1a07e7","id":5}
                  ]
    - Combine the collected data to the appropriate folder style (DONE)
    - Upload the file to Gdirve 
    - Learning how to debug NN (Notes from Andrews Ng course)
        - When do I need more data?
            - Realtionships
                - DatasetSize Proportional to Training Error---(Platoes) (Starting to learn fitting)
                - DataSize Inverse Proportional to Validation Error (Platoes) (After learning from a big dataset)
                - DataSize not related underfitting/High bias
                - DataSize Proportional to overfitting/high varience
        - Increasing or decreasing the lamda?
        - Steps:
            - Divide the data
                - training (0.7)
                - test (0.3)
                    - Compute error on test set
            - Bias vs Variance 
                - High bias/underfitting
                    - Not enough features
                - High varience/over fitting 
                    - Too good in training set but bad at validation 
                - Regularizattion lambda
                    - Too low lambdaL: High varience/overfitting
                    - Too high lamda: High bais/underfit (Dont punish the weights too much)
                - CHOOSING THE RIGHT LAMBDA!
                    - Do a lambda/penelaty test
    - Use smaller images weigths to train on bigger images
        - Canvid: orginal size= ([720,960])
    - Model is training....... 
        - Do the stage-2 (DONE)

Try predicting on a image with the unetmodel (COMPLETE)
    -Useful links:
        - Converting Imagesegment to opencv: https://forums.fast.ai/t/converting-imagesegment-to-opencv/57838/2
    - Load the exported mdel
        - Requires you to define the accuary function acc_courtSeg
    - Prediction test
        - Predictin using the source size?
            - It returns me a image segment object and a tensor and a matrix
                - (y, pred, raw_pred)
                    - Line383 : https://github.com/fastai/fastai/blob/master/fastai/basic_train.py#L395
            - How do I use this output? (DONE)
                - how does learn.show_results work?
                    - The learner is a unet leearner, the show_results function be different from the cnnn ones
                         - ds.x.show_xyzs(xs, ys, zs, **kwargs) is called 
                            - What is xs ,ys , and zs??
                                - xs is a list of: ds.x.reconstruct(grab_idx(x, i)) object
                                    - What does reconstruct do?ww
                                        - transform a pytorch tensor back into an ItemBase. 
                                        - the opposite of calling ItemBase.data.
                                        - What is an ItemBase?
                                            - Something that store the tensor
                                        - What does this function return?
                                             - returns
                                                - Image(t.float().clamp(min=0,max=1)) 
                                                - ImageSegment(t)
                            - test_img.show(y=x[0], figsize=(15,15))
                                - seems to work.
                                - x[0] is what we want
                                    - x[0].data gives us the class mask (THIS IS WHAT WE NEED)
                                        - How does it color the mask?
                                            - x[0] has the same effect as openmask()
                                                - This is because openmask returns ImageSegment object
                                                    - Has save() function
                                                        -  The saved image is in [0..1,]/class mask version
                                                    - Has show() function. 
                                                        - Line 238:
                                                            -  https://github.com/fastai/fastai/blob/master/fastai/vision/image.py#L224
                                                        - Is where the class is turned colourful.
                                                        - calls the show_image function
                                                            - Line 431: https://github.com/fastai/fastai/blob/master/fastai/vision/image.py#L224
                                                            - show_image(self, ax=ax, hide_axis=hide_axis, cmap=cmap, figsize=figsize, interpolation='nearest', alpha=alpha, vmin=0, **kwargs)
                                                                - cmap: 'tab20'
                                                                - alpha: float0.5
                                                                - interpolation='nearest'
                                                                - vmin=0
                                                                -This function first converts the to something else
                                                                    - using image2np(image.data
                                                                        - converts the tensor image to numparray image
                                                                - Applying cmap is how it coverts [0...1] to coloured image
                                                        - Summary:
                                                            - openMask(file)
                                                                - openImage
                                                                    -Line 393: https://github.com/fastai/fastai/blob/master/fastai/vision/image.py#L224
                                                                    - show_image
                                                                        - What the fk is plt
                                                                        - return an AxesIamge
                                                                        - Convert the Axes image to opencv image 
                                                        - convert the image to RGBA to RGB (DONE) 
                                                            -  Turn it into BRG
                            - NEXT: READ the post from Pritesh Gohil
                                - https://stackoverflow.com/questions/28757348/how-to-clear-memory-completely-of-all-matplotlib-plots
                                    - Write a functions that reset the figure size
                                    - Write a function that applys cmap given an image mask
                            - Apply wls-filtering to import the prediciton (STOPPED CHASING)
                                - You can't because you need left and right image
                            - Try on a video that was not in the trianing or validation set
                                - (WORKING YEAH!)
                            - 





        - Producing the mask image in notebook (FAILED)
            - What is the most effiecent way to colourmap
                - Well we pass in img to show_image() it produce colour full image
                    - Look at the source how it produces the colour full image
                    - It passes it to AxexImage(self...) // slef refers to the [0..1] image
                    NEXT: I have Array of class mask. I need to apply cmap to it and get new coloured array.
                        - Try checking fastai fourm if we can save the colour map image
                            - Found nothing
                        - Using the function found in stackoverflow
                            - https://stackoverflow.com/questions/52498777/apply-matplotlib-or-custom-colormap-to-opencv-image
                            - What does scalarmappable do?
                                -  scalar data to RGBA mapping
                                    - because the are all in range [0,1,..5], they all get mapp to blue
                                    - what does scalarMappable.to_rgba do?
                                        - what does np.linspace do?
                                            - produces lineraly space numbers 
                                        - What does cv2.lut do?
        - Producing the mask image in notebook(DONE)
            - I have the img as np.array(...)
                - Can I use fastai to return me AxesImage?
                    - I can use show_image but it accetps an ImageObject. It then extract the Image data to send to matplot 
                        - I can send it the ImageSegmentOject as ImagObject but then I have to convert the data tensor to float.
                            - Does ImageSegmentOBject allow me to change the data to a new data of float?
                                -...
                - Can I just use matplotlib 
                    - What does imshow return?
                        - AxesImage
                        - Bbox image
                            - How does show_img make axesImage
                            - Next look at this: 
                                - https://stackoverflow.com/questions/13623301/convert-contour-matplotlib-or-opencv-to-image-of-the-same-size-as-the-original
                                    - Get the image with out plotting it the image
                                        - How does im_plot.make_image know which image I am talking about?
                                            - I am telling it in plt.imshow. And plt.imshow must returns image.axesImage    
                                                - How does plt.imshow make the axesImage?
                                                    - It calls gca().imshow()
                                                        - What is gca()?
                                                            - returns an axiesimage object
                                                            - the imshow() function of axesimage object is returned
                                                                - Does that mean axesimage imshow function accepts array?


                                                       - How does gca return me the axes image?
        - Make a fuction that takes mask and returns the usable image              


Team detection (COMPLETE)
    - I need to a model that does human detection
        - Human detection Now working
    -Filtering the Human detection, by box ratio, area then filtering these people for team (DONE)
        - Only draw the prediction if the following conditions satifies
            - Box size
                - Find the area of the box
                    - What does box from prediction contain=[left , top, width ,height]
                        - what does each element stand for?
                            - top left coord : (left,top)
                            - bottom right coord: (left + width, top + height)
                        - Yeah, you can just do width * height
                - Find the area of the image 
                - boxPercentage: aereaBox/area_img
                    - <=0.5
            - Label
                - DONE
        - I want it suh that at any given time (HOLD) 
            - The number of detection is 11 
    ROI color extraction
        - Extract ROI
            - Crop the image 
                - easy list slicing
        - Process the ROI 
            - Find the team section
                - max_ratio
                    - We only know the for sure the team color if 
                    - max ratio is greater than some threshold
                    - How do we get max_ratio?
                        - its the maximum value of the ratioList
                        - What is the ratio list?
                            - Its an empty list that gets filled 
                            - What is it filled it?
                                - it is filled with ratio
                                - ratioList = [fractionOfPixelsInColourClass1 , fractionOfPixelsInColoutClass2]
                                - What is the ratio? (DONE)
                                    - color_pix/tot_pix
                                        - numberofpxilesthatfitthecolorcertia/totoalnumberofpixles
                                        - In simple terms: find the ratio of pixels that find into your 2 colour classes
                                    - what the fk is color_pix?(DONE) (RETURN HERE)
                                        - count_nonblack_np(output)
                                            - in a cropped region of a white player:
                                                - There still exist black pixels 
                                                    - If the pixels are black that means we threw them away
                                                - Simple terms: 
                                                    - it returns number of pixles that falls in the boundary
                                        - number of non-black pixels in the image
                                            - returns the number of pixles that were in the selection
                                            - 
                                        - Are we still talking about cropped image?
                                            -NO we are talking about:
                                                - cv2.bitwise_and(image, image, mask = mask)
                                                    - What does bitwise_and do?
                                                        - mask contains YES region. (YES region = white area)
                                                        - bitwise_and: in a given image keeps the YES regions only
                                                    - what is the image?
                                                        - this is the cropped image
                                                    - What is the mask?
                                                        - cv2.inRange(image, lower, upper)
                                                        - What does cv2.inRange(image, lower, higher) do?
                                                            - thresholding based on range of pixels instead of 1
                                                            - Because hue tell use what color its with just scalar value. We use H thus HSV
                                                            - How does she get the threshold? (DONE)
                                                                - For each color there is a threshold
                                                                    - eg. color: (lower_bound , upper_bound)
                                                                    - white: ([0,0,200], [255,255,255])
                                                                    - black: ([0, 0, 0], [255, 255, 60]) 
                                                                    - Where do these numbers come from?
                                                                        - What do these numbers mean?
                                                                            - [lowHue,loewSaturation,lowValue] 
                                                                            - [highHue,......]
                                                                        - Why is the lower bound for white 0?
                                                                        -Experiment
                                                                            - [0,0,0] , [0,0,0]
                                                                                - All black
                                                                            -[255,255,255] , [255,255,255]
                                                                                - only the whitest area is YESED
                                                                            - [0,0,0] , [255,255,255]
                                                                                - All blacked
                                                                                - WHY?
                                                                                    - 
                                                                            - [1,1,1] , [255,255,255]
                                                                                - All yesed
                                                                            -  mask o yellow (15,0,0) ~ (36, 255, 255)
                                                                                - Is it yellow:
                                                                                    -(36,...) or (...,36)
                                                                                    - ITs: (36, 255, 255) ~(BGR)
                                                                                -White
                                                                                    - [0,0,200], [255,255,255]
                                                                            - H: 0-179, S: 0-255, V: 0-255.
                                                                    - The followin link teaches use how to set the boundaries in HSV
                                                                        - https://stackoverflow.com/questions/10948589/choosing-the-correct-upper-and-lower-hsv-boundaries-for-color-detection-withcv
                                                                        -User: Knight

                    - When do we turn the image into HSV
                        - The image we pass into inRange has to be HSV
                    
                    - I have to detect the teams for each bounding box 
                        - It will save time to only detect the teams for the bounding boxes we are using 
                            - So do it after checking bbox fitting (inside the if loop)
                                - But before drawPred, because we want to be able to write the teams 
                                    - if(bbFit)---CodeForTeamDetection-----drawPred()
                                        - Wwrite detectTeam function that takes in box coord 
                                            - Finds the ROI
                                            - Cacaulatest the ratio 
                                            - Compare the ratio
                                        - How do I test detectTeam function
                                            -
                        - 
NEXT: Finish reading detect_team function at.. and finish the "What is the mask thread"
    - https://github.com/priya-dwivedi/Deep-Learning/blob/master/march_madness_team_shot/object_detection/March_Madness_Object_Detection.ipynb
    -https://docs.opencv.org/3.4/da/d97/tutorial_threshold_inRange.html



Ball Regression
    - Trying seeing if you have turned convert the image to RGB before you are passing it to the detector?
    - The image regresion model is up right bad. Because the dataset that I trainon was really bad
        - Remake the dataset.
            - What format should the test and validation set be in?
                - The file name are the coords
                - eg. 9,10.png
            - Find image point labeler tool 
                - Predict 2 points
                    - The shooting player
                    - The ball 
                    - In THAT order!
        - Train the model
            - I have to rename each file 
                - humanx, humany , ballx, bally.png
                    - exatract the information out of the xml files 
                    - 

Debug:
    - CUDA out of memory. Tried to allocate 24.72 GiB (GPU 0; 7.43 GiB total capacity;(SOLVED)
        - Resize/crop/pad the training image set to fit the standard sized used in the pretrained model you are using. Ex: 224x224 for Resnet, 299x299 Inception etc
        - Look at how to resize the dataset
            - Fix: The accuracy function was not great

    - figureImage.make_image() does not work (SLOVED) 
        - say: 'str' object has no attribute 'dpi'
            - fac = renderer.dpi/self.figure.dpi
            - It make sense, because renderer is a string
            - How does axesimage makes its renderer
                - It does not use the rendere varaible
                - What is a rendere? It not a dictionary because we cant do rendere.dpi 
                    - My guess is it is some kind of object
                        - Made an renderer object and passed it down

    - Debug
        - Left is a negative number
            - Which frame is this?
                - 496
            - Does it run sooomthely run when you are not detecting the teams?
                - YES!
                - Why doesn't the negative left cause problem in the case?
                    - Does it still exit?
                        - Yes, it still exit
                    - Try: Getting the coords of that frame, use draw function to see wheer that box is in the frame
                        - -4,966,90,1075
                        - left,top,right,bottom

    - Debug
        - All the teams are either Team0 or unertain
            - How does the masked image look like?  
                - getColorRatio is the function that is making te oi  masked
                    - For the seecond color everything is black
                        - meaning nothing stafies the given color range
                    - Try: anthoer image 
                        - See if the methhod for choosing color is spot on 
                        - with:  ([115, 150, 0], [125,255 , 20])
                            - it is picking up whatever green is 
                                - The range I have specified maps to color green in RBG
                                - So what might be happening is:
                                    - I pass in HSV image -> Th court lines truns to green -> cv2.inRange for that BGR color
                    Try: I tried creaeting mask 
                        - The mask is faulty for that color 
                            - The bounds i tired for colour blue: 
                                - np.array([115,150, 0]), np.array([125,255 , 20])

                        - Some other guys need to select blue 
                            - My mistake:
                                - Having a very small saturation bounds 
                            - Learning point
                                - cv2.bitwise_and(img , img , mask=mask)
                                - Does not change the image, it just uses the shaped of the dest 
                            - Another Big mistakes:
                                - I am thinkg if TEAM1 has colour red in RBG space 
                                    - it will have the same colour in HSV space
                                    - AND I AM passing in RED HSV value
                                - IN HSV TEAM1 have this buleish colour
                                    - So I have to pass in the the bule HSV colour
                                        - Example: 
                                            DARK RED:167,172,55
                                                - Dominiat color: ~167 H
                                                - Satutauration: ~172
                                                - value: ~55 (Lower the value, darker the color)
                                            - Lower bound
                                                - To the left from starting point:H
                                                - To the top from the starting point:S
                                                - lower number: V
                                            - Upper bound
                                                - To the right from the starting point
                                            - Learnt: Having a greater H range help picks colour 
- Debug: 
    - the seoncd loop
        - The roiMasked is totally black 
            - That is not what I wanted 
                -roiMasked = cv2.bitwise_and(roi,roi,mask=mask)
                    - is roi the same for thefrist loop/?
                        - No it is not the same
                        - ing show == True
                            - we have: 
                                - roi = cv2.cvtColor(roi, cv2.COLOR_HSV2BGR)
                                    - Now roi is a BRG img
                                - so we we go back in the start
                                    -  mask = cv2.inRange(roi , np.array(teamColorLower) , np.array(teamColorUpper))
                                        - we are passing a brg image
    - In that region
        - Calcualate the numbers of pixels actiavted
            - is this the same both?
                - NO:
                    - totalPix = countNonBalckPix(roi)
                        - What is roi?
                            - I hope it a rectange of HSV
        - Calcuate the number of pixles activated for T1
        - Calcuate the number of pixles activated for T2
            - 
- Debug: (DONE)
    - Even though the mask is masking the right regions 
        - I am not getting right preds
            - For teamwhite: totalColrPix:429816 , totalPx:1205325
            - For team red : totalColrPix:210525 , totalPx:1205325  
    - Big assumption that I was making
        - Background colour radically differnt form the 2 teams color
            - I assumed that the HSV color i gave 
            - Blacked out the background. 
            - But it did not.


Now I have courtSegmentation model and Posnet model
- I want to combine them both to produce a new dataset
- Train a classifer on it
- Which files were used to make the stickfigure dataset?
    - ActionClassification3 Videos
- Add the courtSegmentation model to the code 
    - I need stick  figures on a court mask
        - Given a frame the
            - Get the court mask
                - Import the court model
                    - There are extra code that will run on import
                        - Make a copy and in that copy only keep the things you want
                        - How do I import a ipynb file? 
                            - Looked at the CV work
                -  Pass courtSeg model the required frame an get out up
                    - Test that it returns what I want
                    - In timeCompression function 
                        - instead of psasing in blank iamge 
                    - What happens when you add a black image to a image
                - CourtSeg.getCourtMask returned mask is not the size of origninal image
                    - So shapes off the diferent images are as follows
                         - recall: 
                            - .shape (H,W)
                            - resize( (W,H))
                         - mergedImage.shape : (1080, 1920, 3)
                         - courtMask.shape : (720, 1280, 3)
                    - It size the size of 720 X 1280
                        - What is H x W
                            -
                    - This means I have to do 2 things
                        - When merging the court mask, merge it with empty image 720 x 1280
                    - I need the mergedPose to be the same size 
                        - So Before I send it off for pose detection
                            - Convert it to that 720 x 1280
            - The court is really dark
                -  Try stacking court seg in datamoe and and a raw pose frame
                    - stacking: [background] on [pose] - gives brighter imag
                - The reason it was so dark because I am only stacking 9 imagges
                - The Stick figure seem to lose their color if I use
                    - cv2.addWeighted()
                    - Masked I loss some details
            - Using unstacked court?
                - 

            - May be only use 1 colur for the poses
                - Color code the poses by team?
            -

            - Get the stick figures
        - Put the figures on the court mask instead of an empty black image

Run the free-throw dataset and the DUNK
Where to go next?
    - what are my goals, have I completed them all?
        - Ball detection
        - Action Classification
            - Pose detection
            - Court Segmentation
        - Team Detection
            - Object detection
                - Can I improve this?
                - Filter out the false positives
                    - Refs and Spactators
        - Producing team stats

Team Detection with K-means
    - https://github.com/stephanj/basketballVideoAnalysis/blob/master/color-detection/show_colors.py
    - Steps
        - For each image file
            - Convert it to RGB
            - Reshape it 
                - NumberOfPixles x Features(Channels)
            - pass it Clutersing Algorithm
                - Will find: clt.cluster_centers_
                    - c1:[] , c2:[] , c3:[]
                    - A pixel will go in to one these clusters
            - Find histogram
                - Find the percentage for each cluster
            - Plot bar chart
                - 

- Idea
    - Only get the pose of the player at the found bounding boxes?
    - Change from yolo to mask-RCNN


Ensembl learning?
    - https://www.analyticsvidhya.com/blog/2018/06/comprehensive-guide-for-ensemble-models
IDEA:
    - Action classification done only using Pose stuffp
    n
        - Using K means clustering?
            - 

Team-detection
    - Swap yolo for mask RCNN
    - Use k-means clustering for for pixles 
        - To find the most common color for the pixels
    - Apply k-means clustering for image colours
        - Team0 , Team1 , Others
    - Write a detectTeam() function
        - I want to ouput a team value
            - Find the dominminat colors using clustering
                - Compare the dominant color with 2 primary colors
    - Write a function that will do basic RCNN team detection (DONE)
        - Edit the detectTeam Basic function to handle FASTRCNN
            - getRoi
                - it gets roi using 
                - update the roi using a mask
                - Can I send boxes to postprocess?
                    - is boxes == results?
                        - results is a grid
                            - row/results col/detection
    - Write a function that will do FasterRCNN with K-Means 

Debug
    - we are setting image[startY:endY, startX:endX][mask] as roi
        - image[startY:endY, startX:endX][mask] = (4667,3)
        - What we want is normal bbox as the roi but region outside the mask is 0
            - What does image[startY:endY, startX:endX][mask] return?
                - Returns all the pixels in the mask
Debug:
    objc[62273]: Class RunLoopModeTracker is implemented in both /Users/sandeep/opt/anaconda3/lib/python3.7/site-packages/cv2/.dylibs/QtCore (0x11a4e97f0) and /Users/sandeep/opt/anaconda3/lib/libQt5Core.5.9.7.dylib (0x11ff98a80). One of the two will be used. Which one is undefined.

    QObject::moveToThread: Current thread (0x7feb5089c590) is not the object's thread (0x7feb50d2fe60).
    Cannot move to target thread (0x7feb5089c590)

    You might be loading two sets of Qt binaries into the same process. Check that all plugins are compiled against the right Qt binaries. Export DYLD_PRINT_LIBRARIES=1 and check that only one set of binaries are being loaded.
    qt.qpa.plugin: Could not load the Qt platform plugin "cocoa" in "" even though it was found.
    This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

    Available platform plugins are: cocoa, minimal, offscreen.
    Fix: python 4.2 not stable


Action Classification 
    - Learn to use weights and bias
    - What metric do I use for imag classification
        - Explaination for Percison and Recall for muticlass classification
            - https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2
            - How to get precision and recall score for all the classses?
                - https://www.kaggle.com/dromosys/fast-ai-news-multi-classify-v2

    - Why do we need to use random seed
        - https://medium.com/@ODSC/properly-setting-the-random-seed-in-ml-experiments-not-as-simple-as-you-might-imagine-219969c84752
            - What was the change from?
                - Acutal model
                - data modification
                - result of new random sample
            - Same seed == same starting point == same sequence of numbers 
            - When to set the seed?
                - You have to run random_seed(0), before the first fit;
                - You have to run it before creating the databunch;
                    - #Remember to use num_workers=0 when creating the DataBunch.
                    - Setting random seeds before creating the DataBunch is needed to have a consistent Train/Validate split.
                - And you have to call it every time for each different time you call fit.
                    -  the creation of the new learner that reset the random seed.
    - See if you can do action classfication using pose only
        - Sometime there are enlongated arms
        - See if using openpose yeild better reuslt
            - The looking and running a unknown model take a very long time 
            - Hence, I am not doing this
        - Making ovservation
            - Player is detected
            - Player is not detected
            - Theere are front shots where pose of spectators are detected
            - Notice any Clustering?
                - 
            - Look at the Ref?
            - Is there any pattern in the their Arms 
            - Direction the player is facing?
                - The bent in their joints?
                    - The direction player is looking at is always the ball?
                    - The position of the ball
                        - Imagine if ther rays coming out of the detect players eyes
                            - Does the intersection of these points tell me a estimated position of the ball?
                                - Its ture that the ray must be deflected by some degree.
                        - I have this idea, Reducing the region of search. 
                            - Try drawing a line from ear to eye 
                                - Do this for every player
                                    -  get_adjacent_keypoints
                                        - Returns 2 points from the player joints where the line needs to drawn
                                    -  Get the coords in the line
                                        - Retun 2 points from the player joints (left ear , left shoulder) and (rE and rS)
                                    - I hav to filter out lines 
                                        - Why do I want to filter out some lines?
                                            - Some lines are just absurdly wrong
                                        - Can we filter some lines using the slope?
                                        - Can I use the angle between lines?
                                            - What does the angle between 2 lines tell me?  
                                                - The shooters don't neccesarry have the smallest cone angle 
                                                -  the slope of the line does tell me where the player is facing 
                                                    - There are players who have general facing direction
                                                    - Next find the general facing direction(DONE)
                                                        - Now I know where the players are facing
                                                            - This does not help me tell extacly a thhings is 
                                                                - The possilbe location of the ball?
                                                                    - Closest player in that direction probably has the ball
                                                                        - There could be multiple players in that line of sight
                                                                        - Does the hold the ball in the line of sight have opposite line of sight?
                                                                            - The person that is being looked at the most?
                                                                                - Where is this mistery persion
                                                                                    - Left or right?
                                                                                        - if the distance from start point to end point is positive 
                                                                                            - he is looking left
                                                                                            - else right 
                                                                    - intersection of the line sight act as possible location of the ball?
                                                        - There are outliers
                                                        - Look at bunch of clips where people are shooting
                                                            - I notice defending shoot needs full arm up
                                                            - I notice when player shoots the ball there is a net change in y-direction


                - Why am I doing this?
                    - I already have the team
                        - what is my goal?
                            - I am searching for any insight that I can bring to Action classification or ball possition by observing the way players
                            - May try doing detection  footage that are stable with the camera?    
    - For each detected pose
         - Each human being what are they doing 
            - So I need a way to label the poses for each human being?
                - In the mirror project
                    - They collected full body images of different movements
                         - stored the poses
                            - Make these poses comparable by
                                - crop the image using bounding boxes
                                - rescale it to constant size
                                - apply l2 normalization to the key point coordinates
                            - How was the poses stored?
                                -  1D array: [x1,y1,x2,y2......]
        - My idea:
            - Big picture:
                - Make a dataset (DONE)
                    - X = (person X pose)
                    - Questions:
                        - How to collect the data?
                            - Collect poses of each individual humans
                                - I can a pass a frame
                                    - Get  all the poses for the humans
                                        - Detect all the humans
                                            - YOLO
                                                - Will posenet work well with croped images?
                                                    - The cropped image is very low res
                                                    - There are mutliple human being detected
                                                        - How Do I isolate
                                                            - Take the biggest
                                            - Use posenet 
                                            - Manually crop 
                            - Catogarise them 
                                -
                        - How to take in account the confidence score?
                - Figure out which which points refer to which
                    - {10:'left_ankel' , 13:right_ankel , 9:'left_knee',12:'right_knee',8:'left_hip',
                       11:'right_hip', 4:'left_wrist',7:'right_wrist', 3:'left_elbow',6:'right_elbow',
                       2:'left_shoulder',5:'right_shoulder',1:'nose',17:'right_ear',15:'right_eye',
                       14:'left_eye' , 16:'left_ear'}

                    -[,,, , ,
                     , ,,, 
                      ,,,,
                      , ]
                    - There is a porblem 
                        - PART_NAMES contain 17 points 
                        - While x[0] contain 18 points
   







Debug:
    - I only condsidered the case when blue line has greater angle
        - what color does theta1 refere to?
            - Theta1 == bule
            - Cosidering for 
                - B: -0.18216831047276347 , G:1.5707963267948966  
                    - Is -0.18 == 0.18?
                        - (-0.18) == -10deg
                        - (0.18) == 10deg
                - I should not be using argMax
                    - It is easier to look it as blue and green line 
                - Can we work in degrees?
                    - What does  theta1 = np.arctan2(p1[1]-p2[1], p1[0]-p2[0]) return?
                        - returns radians
                        - when does it return me negatives?
                            - 
                    - np.cos()
                        - takes in radians
            - I dont know which agnel corrppondes to line
                - write the angle in the image interms of deree
            - Look like I have to figure out which quadrant the lines lie in
                - If they lie in the same quadrand  
                    - theta is just half the ang
                - I know how much to from green line
                    - I don't know which direction to move in 
        - Because a cone starts from 1 point
            0 



DEBUG
    - Find the angle of the two line from the atan2 line 
        - Each line angle ranges from [0-360]
    - Find the angel between them = alpha
    - Add alpha/2 to the line with smaller angle?
        - Wont work because if the smalle line in the the quad4 and bigg liner is quad3 
            - add to smaller liner will make it go close to bigger line 
    - Ad alpha to line with who ever has smaller arc tan



DEBUG:
 -  If the point is not visible, then it does not have the point

np.array([
    [ keypoints[left]['position']['x'] , keypoints[left]['position']['y'] ],
    [ keypoints[right]['position']['x'] , keypoints[right]['position']['y'] ]
        ]

        - does it append the first point if the second 1 does not exist?
            - No because it will fail to create the array
        - x[0] == keypoints
            - x[0]: {1:()}
        - eg : keypoints[poseKeyInX][x|0ory|1] ,
- poses == x
    - [{pose}] only 1 inside
Drawing humans (DONE)
    - cv2.circle(npimg, center, 3, common.CocoColors[i], thickness=3, lineType=8, shift=0)
        - Draw a cricle for the keypoint
            -keypoint: ()
            - How to get the color for the keypoint?
                - I have the key for the point eg. 17
        - center is the cooord 
        - color is the Colorst ther pose 
        - Drawing line 
            - Getting the adjacent points
                - What does adjacnet_keypoints continas?
                    - When we extend a array of array does this differe from the normal one?
                        - The colours are are in RGB convert it to BGR
        - What are the sizes of the circle and the lines
            - circleWidth = 6;
            - lineWidth = 8;
            - drawSegment([ay, ax], [by, bx], color, scale, ctx)
            - drawPoint(ctx, y, x, r, color)
                 - what is ctx?
                    - is the objects that draw it
                 - What is scale?
                    - Not nedded
Draw the keypoints in the same 
    - The porblem is in the real video
    - I will not have access to the pose of the person every single frame
Action classfication
    - Goal:
        - Action classification 
            - single person pose take the majority action that is not walking etc
        - More about the goal:
            - If I have a model that can classifify action using 1 image for everyperson
                - Then in the demo, I can run for each detected person I can crop the part
                - Draw their pose and send it to image classifcation algorithm
                    - This will mean that I will have to classify for many many humans about 10
                        - This will be a interesting comparison between action classification(GOAL)
                            - Single person pose (x10)
                                - More accurate may be?
                                - But may take more time
                                    - To imporvove time may be 
                                        - We can use cosine similarity measures?
                            - A image with all the poses
        - I forgot that each imge had to be named differently
             - I forgot to check the sizes of each generated folder
            - 
    - Project Ladder
        - Be able to draw skeleton(DONE)
        - Draw skelton on white background
        - Do I scale the image to 244,244 imagenet size??(FAILED)
            - Lets do 512x512,Because we trained the model on 512x512
                - The semnatic of the images changes
                - 
            - Make a empty 244,244 image
            - I need a function to map coords to the 244 coords
                - I have this function, but dont knwo where
        - Save the images
        - Upload the images to the drive   
            - Currenlty, uploading.........
        - NEXT: Train Resent on the images
            - Merge the files
                - Do a test to move a smalle file from 1 folder to another 






































    - Project Ladder 
        - Goal:
            - Find the vector that is most simliar to the pose 
                - Use the classs of the pose as the predicted actions 
        - Collect datset
            - 1 human pose is an array
                - I need a dataframe that has all the keypoints 
                - df.append(poses_dict, ignore_index=True)
                    - There is a problem with this 
                        - some poses_dict: {'nose':()...}
                        - some poses_dict: {'head'} // does not contain nose
                            - if the index is not there nan is placed
                                - What do I replace my nan values with? 
                                    - Either we can use proxy
                                        - I am using a poroxy
                                            - Use the location of the last connected part
                                                -  
                                    - Either we can (-1,-1)
            - Maybe work with dummy dataset
                - Rescaling and Resizing 
                    - All the images are of the same size, thus, not rescaling needed
                - Applying L2 normalization to the dataset
                    - Because the imae is not a sqaure, would applying an l2 regularization be approizate
                        - I believe mapping the image to sqaure size would do no harm, because
                     


                     - Write a function that does L2 normalization on the main df
                        - I have to spread each tuple in the datafram
                            - Change the way I load my data to you have 1_x,1_y etc
                                - Is is best for me to check nan along the rows or cols?
                                    - get index in unconnnected that is non non
                                - df.apply(function)
                                    - the function gets the the row 
                                        - get all the keys with nan
                                        - search for the poxry key
                                            - make a dict for keys to df_keys 
                                            - The porxy key can be in UNCONNECTED_INDICES OR CONNECTED_INDICES
                                                - We dont need to make connected indices, we can check if in unconnected indices
                                                - If in UNCONN
                                                    - filter through the index that are in key //aka list key of nans
                                                        -usable_unconnected_key
                                                            - we can just use the first element
                                                - If in Connected 
                                                    - use its closest pair that is available











                                        - set the keys x and y value to porxy key x and y
                            - convert the coord to 244 x 244 space
                        - Apply l2 normalization
                            - 
                        - Save it as csv
                    - Applying Cosine Simliarity measure
                        - This article explains consine distacne
                            - https://cmry.github.io/notes/euclidean-v-cosine
                            - cosine
                                - converts the vetors to unit length
                                - And compare their agian
                            - eucledian 
                                - raw distance
                        - Vantage point trees
                            - write a function to build a tree
                                - pass in the data 
                                    - remove the class columns from the data
                                    - The data does not contain calss 
                                        - edit the dataset to contain the class 
                            - Write a functions that takes in poses and returns poses for each human
                        - Test Action Clasfication using Cosine
                            - For each pose I get I need to generate a pose vector
                                 - ActionClassificatonCosineDataGen file read a vector, maybe I can 
                                    - pass the poses to the file 
                                    - get correct poses vector
                                        - change the keys
                                            - what should they keys be changed to?
                                                - nose -> 0_x and 0_y
                                                    - Map the nose to key relative to raw dict
                                                    - get the extended_pose

                                                    - Are all the posenet bodyparts in the second file?
                                                        - unknown is not there
                                                        - So that mean I will have to find porxy coord for unknown

                                        - that is relative to image 244*244
                                        - the vector is a list that is in the correct order
                                        - porxy coords filled?
                                            - 
                            - The database is just too big 500,000 imges
                                - Write a fucnction that track the time spent searching for tree
                                    - Mean search time for 500,000: 7.012 seconds/pose
                                    - Mean search time for 169,530 : 4.12 seconds/pose
                                    - Mean search time for 116,552 : 3.721 seconds/pose
                                    - Mean search time for 37,085 : 1.804 seconds/poe
                                - Cut the dataset size only use the middle frame
                                    - How do I identify the middle frame?
                                        - Take the 7th frame
                                        - What do I do with the labels 
                                        - get_data() 
                                            - outputs data with labels
                                        - Just do modules operation and keep every nth index
                                            - the index start from 0
                                                - keep 6th, 7th , 8th index 
                                                - 
                            - Write a function for weightedDitaceMatching
                            - Prepare the dataset 
                                - Load the dataset
                            - Prepare the poseVector
                                - Not l2 normalized
                            - Prepare the theta vector
                                - [0_x,......]
                            - Build the tree
                                - I need weigths?
                                    - Make the theta 1 for the original data
                            - fit()
                                - Reshape the data
                                    - stack it horizonatlly so that is (10*72)
                                        -[coorrd.....weights....]
                                - in the weighted function
                                    - check if we using this function for making the tree or for prediction
                                        - predction we will have the second dimension as 72
                                        - making a tree we will just have 32..but we dont have to check for this 
                                    - if prediction
                                        - we want to reshape it to (2x36)
                                        - poseVector = pv[0]
                                        - theta = pv[1]   
                            - Draw 


DEBUG
    - [0_x,0_y] is not in dff
        - What does 0 refer to in raw?
            - unknown: Hence, we know that for unknow we should always need to fill it with proxy coord
        - The dataframe should already contain nan values after getting passed down which function
            - Before filling the NaN values 
                - Get_extended_pose 
                    - no: this function extends whatever the poses_dict has
                - get_data()
                    - this function assure that keys that are missing are filled in with Nan
                - When you create a dataFrame it will automatically 
                    -  df = pd.DataFrame(data)
                        - It automatically fills in that 0_x 0_y 1 of any poses have it 
                        - But because in the posenet results al lthe the poes dont have the key unknown 
                            - hence wehen creating the data frame 0_x and 0_y goes missing 
                            - I need to have 0_x 0_y because 
                                - Original data is made up of vectors that contaon 0_x and 0_y
                                    - Vptree is built of the vectros
                                - Later I am planning to use wighted sum 
                                    - I will need an apporxy weighths for the unknow 
    - index 9 
        - has nan value
            - why is index 9 nan value not filled 
    - ValueError: setting an array element with a sequence.
        - getSimilarPosesClasses

Debug
    - poseVector1 is not getting passed properly
        - pv[0] is being passed
            - getPred
Debug 
    - All of the prediction are dribble 
        - Check if this is right 
            - call findMatch() directly?
                - Cool I get the similar pose striaght to me 
                - How can obtains the class 
                    - findMatch is returning me that same array
                        - The score is really high
                        - Who is returing the findMacth?
                            - the tree
                        - Check the similarity distance function
                            - got the eqation wrong
                            - try unvectorised format
                                - You have v1,w1 and v2
                                    - Try doing it with for loop like in the post 
                                        - If you get the same anwer that mean you hab gotten the eqaution right
                                            - The stack is working correctly
                                            - p1 and v1 are 100% correct
                                - I can conform the the equation is correct
                        - I can draw the poses
                            - I have a list 
                                - [0x,0y.....]
                                - I need need to draw black image 
                                    - draw circle in the x,y coord
                        - Check what size the coords in the main data frame is in]
                            - 244 in get_extended_pose
        - The returns data is not scaled to the correct size
            - generate_dataset() returns me a unscaled data
                - get_extended_pose() called map_function
                    - get_data()
                    - get_posenet_extended_pose()
                        - why isn't 
                    - The error is 
                        - the I am getting the wrong scale factor
                            - it should be from image szie to (244,244)
                            - not from 178,512
                            - How to get the correct image size?
                                - 
        - Draw the poses that was received with the most similar pose
            - Use SinglePlayerPosedDataGen
                - draw_skel_and_kp([poses],frame,[255,0,0])
                    - frame: pass in the frame as an empty black image
                    - pose : 
                        - What format does it accept?
                            - {0:(x,y).....,} 
                    - getScaleFactors(244,244,ARGS['inputImgH'],ARGS['inputImgW']) is called
                        - that scale obtained is passed to getScaleFactors(244,244,ARGS['inputImgH'],ARGS['inputImgW'])
                            - what is value?
                    - Printing stuff gave me 
                        - scale factor :(5.987704918032787, 3.3811475409836067)
                            - Big -> Small
                            - I need to crop the image as well 
                                - So that the pose is big, and that become the out actual image
                                - Then I need to map the image to size 244 
                                - What I am doing WRONG 
                                    - I am mapping the entier being image to 244,244 
                                    - So it pose is taking really small part of the 244 image 
                                        - Imagine this. You are trying to fit A Big image into smaller image
                                            - The small part of the smaller image get even smaller in the new smalle image
                    - Fix:
                        - Use human boundingg boxes to crop the image
                            - I dont think I can get the bounding box straight away (DONE)
                                -  posenet from '@tensorflow-models/posenet'; has the function 
                                    -  getBoundingBox(keypoint) in utils
                                        - get 4 points, how  are theses 4 point found?
                                            - These 4 points are the corners points 
                                            - getBoundBox() finds 2 points of the opposite site
                                                - left_top and right_bottom
                                                    - {minX, minY, maxX, maxY}

                                                        - What are are these points?
                                                            - The four coord are 
                                                                 -  {x: minX, y: minY}, 
                                                                        - left-bottom
                                                                 -{x: maxX, y: minY},
                                                                    - right-bottom
                                                                 - {x: maxX, y: maxY}, 
                                                                    - right_top
                                                                 - {x: minX, y: maxY}
                                                                    - left-top
                                                        - How are thse found?
                                                            - for all the (x,y) of the body part 
                                                                 - for x
                                                                    - compare for maxX
                                                                    - compare for minX
                                                                - for y 
                                                                    - compare for maxY 
                                                                    - comapre for minY
                                                        - For the bounding box coord
                                                - drawpose 
                            - Crop the ROI
                                - Note the structure of posenetPred and bboxes
                                    - {'dectionList':} -> [item,...]/item={}=1human ->{'score': , keypoints: []} -> [{}]
                                    - {'bbox':} -> [item,...]/item=[...]=box ->{x:,y:}/coord   
                                - Take in account of the bound error 
                            - get the coord of the points in the cropped image
                                - subtract it away from the lefttop
                                - to test it draw the keypoints on the cropped image
                                    - find the bounding box coords
                                    - crop the image 
                                         - for each roi 
                                             - find the get the coord for the key points 
                                             - draw circle on the key points 
                            - Now can I properly visualization function now?
                                - Do I need to convert the roi into a constant shape?
                                    - No that the jobs of map map
                                - All the roi is of different shape
                                    - Convert them to 1 consistent shape 
                                        - If I reshape it now what is the point of mapping function later on 
                                        - Some bound 
                                        - Good side of using this method is 
                                            - it follows the current datapipeline 
                                            - I highly suggest not changing the datapipline, because other part may be affected
                                                - Hence, I will be using this method
                                        - To use this method
                                            - The posenetPred need to contain coords 
                                                - That are relative to the cropped image 
                                                    - Scaled to the image size of (244x244)
                                            - At the end of I think I will need a function called
                                                - preprocess(posenetPred)
                                                    - that will take posenetPred and do the things mentioned above
                                                    - Before passing it to the Dataset.generate_data_posenet


                                    - Pass the Dataset.Gernerate_data_posent indicdually
                                        - I am confused about 
                                            -  getScaleFactors(imgW, imgH, originalImgH , originalImgW):
                                              - is called by  getScaleFactors(244,244,ARGS['inputImgH'],ARGS['inputImgW'])
                                              - okay: as long as I know row are the height of the image, everything should takecare of itself
                                              - The poblem is:
                                                - scaleFactor : (imgH , imgW) -> (rows, col)
                                                - keypoints : {'x': 598, 'y': 509}}    -> [x,y]
                                                    - is x the rows?
                                                        - if yest than everything is is okay because [x,y] -> (row, col)
                                                        - if no than: flip the keypoints list before passing it to mapToSquareImage (THIS IS THE CASE)
                                                            - In reality we need to pass : mapToSquareImage([y,x] , (row,col))
                                                                - we get [y,x] by doing  keypoints.values()[::-1]
                                                            - The returned coords
                                                                - [y,x] so we need to 
                                                                - filp is back so that we have [x,y]
                                                                    - set the values to the keypoints

                                                        - How to check?
                                                            - I must have used it whend drawing the points 
                                                                - draw circle expect what?  
                                                                    - (cols, rows)
                                                                - Therefore I know 
                                                                    - [x,y] == (col,rows)
                                        - Write a preprocess function 
                                            - that takes posenetPred and bboxes 
                                            - updates the posenetPred to contain coordinates 
                                                -
                                                - scaleed to size 244 x 244
                                    - Posenet Parameters 
                                        - nmsRadius:80 or 60 (gives good reults)
                                        - outputStride 
                                            - 16(Few poses are wrong)
                                            - - 32 (Poses are straight out wrong )
                                            - 8 (better results)
                                        - imageScaleFactor
                                            - 0.1
                                                
                                            - 0.3
                                                -  slightly imporved poses
                                            - 0.4
                                                - Same results 
                                                - longer time
                                            - 0.5
                                                - same results 
                                                - longer time
                                        - image element/input image size
                                            - making a it a square image 
                                                - 1920 x 1820
                                                    - 
                                                - (600,600)
                                                    - Many more pose are detected correctly in the zoomed in section
                                                    - Few poses that were detected in the 1920 x 1980 are miss-classified
                                                    - it is a bit faster 
                                                - (500,500)
                                                    - Many more possible missclassification
                                                - (400,400)
                                                    - Even worse
                                                - (700,700)
                                                    - A littele las miss classification
                                                - (950,950)
                                                    - Seems to be doing okay
                                        - input resolution
                                            - compared to (300 x 300)
                                                - (200 x 200)
                                                    - Not too accurate 
                                                - (400 x 400)
                                                    - Takes longer
                                                    - look like nsm is not enough
                                                - (600x600)
                                                    - Really slow
                                                    - But good results
                                                    - Look like nsm is low
                                                - (300x300 is best)

Debug 
    - coordinates are not scaling propley from cropped image to 244
        - Am I passing in the un-updated keypoints
            - No
        - mapCoordToSquareImage must not be doing it' job
            -  The first croped image has shape (239,123)
            - The scale factor (244,244) <- (239,123) 
                 - returned as (1.9, 1.02)
                    - looks like it doing (24)
                 - It should  be (1.02,1.9)                                                         










                            - How will know the coordinates of the pose in the cropped image?
                                - The scale os the imae pose won't change 
                        - oringinal: (598.2205014603861, 509.00750141044625) , mapped:[3581, 1721]
DEBUG
    - the passed h,w is set ito ARGS[inputIMgH/W]
        - 


- Posenet Detection is not 
    - Use Mask RCNN
        - Cropped image
            - Do I have a function where, given a bounding box, I can crop the image?
                - I have a function where I sent the 2 coords and get the cropped roi
                    - (top, left) and (bottom,right)
    - There are 2 things that I can do 
        - i can use the bounding boxes 
            -  crop the roi in the bounding boxes 
                - get hold of part of the image that is represented by the mask
                     - I am drawing on the mask part of the image 
                        -  So the draw function will probablly help me
                        -  roi 
                            - contains the image we want 
                                - its generated from detectTeam
                        -
            - Goal: roi that has 
                    - no background
                    - blurred background
                    - original background
            - send it for posenet 
    - Project ladder 
        - Pass the image through Mask-RCNN
            - With the bounding box coordinates 
                - Can I crop the segmented image?
        - In team detection write a function fitMaskRcnn
            - this function return the postProcessed results (DONE)
                - classID, confidence, boxe, masks
        - Do I want to draw the team?
            - Yes, please return me the frame where team is drawn?
                - But then I will need to draw the actions as well later 
                - Maybe I can draw when I have everything at once
                    - I wont have to write new draw function, can just use TeamDetection by adding one more parameter
                    - THat will be have to run 1 for loop at the end again
                        - this same forloop is used to get the roi,team,teamColor ,teamName
        - I want for each detected object 
            - teamName, teamColor 
                - I will need this for drawing the team
            - roi , mask
        - Do everything in the team detection file
            - Option 1
                - Make an empyty image of the same size 
                - Put Roi There 
                - Crop the roi
                    - Resize it 
                - Run posenet on the resized images 
                    - Draw poses on the image 
                - Results 
                     - Posenet doe not detect any poses in the 
            - Option 2 
                - Crop using the bouding boxes
                    - Give padding to the bouding boxes
                - Better Mask-Rcnn is not detecting many players
                    - Checking posenet whole image vs posenet individual pieces
                - MAKE SURE THAT YOU PASS CLEAN CROPS TO THE MAIN IMAGE
            - Apply historgram equalization on the cropped image?
                - 
            - Option 4 Boka effect 
                - Apply guassian blur to the cropped image 
                - Add the roi on top of the guassian image
            - The more different the colour 
                - The better the separtionation between objects
                - The better the detection
            - Frame rate is highly depended on 
                - image size that goes into posenet
                    - 600-500 = 0-2 fps
                    - 450-400  = 4-5 fps 
                    - 350 - 300 = 7 fps

Writing video file 
    - https://github.com/priya-dwivedi/Deep-Learning/blob/master/Mask_RCNN/Mask_RCNN_Videos.ipynb


- I can split the object detetion mod
- Train CNN (DOING)
    - Cl


function ConnectButton(){
    console.log("Resnet50 Button added pushed"); 
    document.querySelector('#cell-i7WWOkHXvJL7').children[2].children[0].children[0].click()
}

- How does wiriting a video work?
    - Have 2 filePaths
        - filename and newLocation
    - Intesiate the VideoFileCilp class
        - Tell VideoFileClip to proccess each frame
            - .fl_image(f) : takes in a function that uses f to proccess the frames
            - 

GOAL
    - Merge team and action classifcaiton (DONE)
    - Render a video (DONE)
    - Create a app using flask to 
        - log the action of the 2 teams
        - Add timestamp 
            - Able to click the time stamp 
                - Clicking the timestamp start the video from that time stamp
    - Test thr range of posenet in the real life


Goal 
    - Test posent in room and see how good it does
    - Integrate the 
        - (Whole image) Action Classification + Gaze Direction
    - Integreate the 
        - (Whole raw image) Action Classification +  Ball Regression 
    - Integreate 
        - (Single pose) Action Classification 
        - (Single pose) Action Classification + Team Detection (1v1)

Writing notes
    - Although .......,however,........

Goal
    - Filtering action 
        - OPTION 1: find how much different is this current pose from the last one 
            - Find weighted distance matching 
            - How to test this?
                - Draw on the op
                    - the distacne of this poses with last one, to get sense of distacne
                        - for the weigthedDistacne function I am expecting pose2vector to be 36 and posevector1 to be 72
                            - I can write a new funciton
                                - Repeating code 
                                - What do I do with theta of posevector2?
                                    - Treat it like the previous pose as the ground truth 
                                    - 

                            - Edit the function 
                                - I will have to do extra computation in the tree search
                                - Might affect the tree
                        - Just pass in
                            - poseVector2 : pervious pose 
                            - poseVector 1 : The new pose with theata
DEBUG
    - DrawPose.draw_skel_and_kp(posesCopy,frame,[255,0,0],classes) is receiving 2 classes 
        - fit is giving me 2 poses
            - I want to be able to draw the classes for each sekelton
                - the draw take the result from posenet, then what is it complaining about?
DEBUG 
    - operands could not be broadcast together with shapes (36,) (1,72) 
        - previousFramePose : (1,72)
            - Who is setting this?
                - it might be because there were 2 poses
                    - What am i going to do if there is 2 poses in the last frame?
                        - And int the next frame there is only 1 pose
                            - Compare it to both
            - Is the last 36 elements of previousFrmaePsoe theta?
                - No: prevPose (2,36) then in weigthed we are reshaping it to (1,-1)
                    -  NEXT: WHAT TO DO, last frame had 2 poses new frame has 1 poses. How to do it such that it can be scaled to 10 players?
                        - What are my options?  
                            - Compare both poses with that single poss 
                                - This is will work for 1v1, will it work for 5v5?
                                    - Write a function that will do get the mean coordnate of the pose
                                        - Compare the mean 
                                    - Write a function that will get the middle coordinate of the bounding box?
                                        - What if it is a new player in the exact same location?
                                    - Even in 5v5, for a single player we compare it's pose with all the other players pose in the last frame 
                                        - We will avoid searching VP-TREEE
                                        - Will the generated action be accurate?    
                                            - If the distance is really LARGE, then we move on to next pose.
                                            - IF the distacne is really SMALL, then we change to that particular pose 
                                                - But you will still be changing the pose
                                        - Using team information (NOT WORK)
                                            - So in 1v1, if we have 2 poses in the last frame and 1 pose in this frame
                                                - Figure out which team
                                                    - Compare the poses with matching team
                                                        - What if team of the both poses are same?
                                                        - Therefore it will not work
                                    - Only check every 16 frames 
                                        - 
Measuring the distnace of pose in last frame and this frame (DONE)
    - Subporblem: I have to know which pose is which, aka, be able to track poses
        - TeamColor
        - Center of the bounding box
            - Get the center of the bounding box , using bboxes 
                - I will be given coodrinate of bounding boxes [(p1,p2,p3,p3),....]
                    - I will have to find the center of the  boxes 
                    - Compare it with cneters from the last frame
                         - We will need to store the center of the last frame 
                    - Return 
                        - action
                            - I have the bbox from the input 
                            - Store the previous freame info
                                - info: {center : action}
                                - What does bboxes look like?
                                    - bboxes: [ [p1 , p2] , ... ] (from getBoxCoord)
                            - End Goal: 
                                - I have a dictionary with centers and action from the last frame, I have another dictionary  with centers, 
                                    - if there exist a closest center, find it 
                        - a key that refers to that poses 
            - How to integreate Tacker in to ActionClassificaton Cosine?
                - The very first frame
                    - I fit for the whole image
                    - set the tracker
                        - classes, bbox 
                - The second frame onwards
                    - Tracker :   
                        - call track : 
                            - should return: {(closestMidPoint):actionClass,....} (Done)
                            - store a dictionary : prevFrameMidpoint -> pose
                            -  For the poses whos' closest mid point is found 
                                - check if they are the same pose by measuring the distance
                                    - True: Replace the ActionClas
                                    - Flase : Do not do anything 
                                - update the the tracker,and prevFramePose
                            - 
    - Testing
        - Draw bouding boxes (DONE)
        - Draw a circle in the previous center 
            - a line from new center to previous cnenter
        - OPTION 2: keep the same action if the min distacne to the most similar pose is greater than some threshold
            - How do I get access to the distance?
                - similarPoses : ([score] , [poses])
        - Which option  is better?
            - Option 2, 
                - we dont have to do more calculation
                - It is more easier, just check the threshold,
                - A good way to test is 
                    - Print the score on top, so I can have look on 
                        - the score changes with the poses
                            - Eg. If the pose change from walk to run
                                - score changed from 20 to 30
                                    - Okay, I looked at the score changes
                                    - The pose change is all between 10 to 13 
                                    - It is not the like the next poses is an outlier 
                                        - It still thinks that the next pose is really good match
                                            - therefore, option 2 won't qork

Goal 
    - Applying Multi-threading   
    - Find ball using gaze 
    - Find ball using regression

Applying Multi-threading
    - Do I want to crop the image 1 by 1?   
    - I want to send the cropped image to posenet simultaneously 
    - I want to also search vpTrees for the poses simultaneously.
    - Following tutorial
        - https://www.pyimagesearch.com/2019/09/09/multiprocessing-with-opencv-and-python/
        - get some important numbers
            - number of proccess
                - proc
            - proccess id
        - create payload 
            - [{},{}]
        - Pool Class
            - Creates the python process of respective core of the processor
                - proc
        - Map
            - Takes payload list 
            - Call function on each correct
                - over input image paths in the payload
        - How to combines results from the payload
    - How do I make a simple test?
        - Do it on image yyyy
            - load image (0   `)
            - Load team detection
                - Pass it through fast rcnn
                - make 10 croped images (DONE)
        - Spawn 10 thread 
            - https://realpython.com/intro-to-python-threading/
            - pass the payloads
                - Try: 
                    - Proccess_image() to call getting pose/action via TeamDetect 
                        - Will each procces spwan new instance of the function and run independely?
                    - 
            - The problem with threading is
                - It does not happen simulatenously 
                - It is still sending request 1 by 1 
        - Multi-thread and use all 8 cores
            - 1 pose
                - 0.226
            - 2 pose 
                - 0.412 per pose
            - 3 pose
                - 0.659 per pose 

